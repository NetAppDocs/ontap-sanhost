---
sidebar: sidebar
permalink: nvme-rhel-10.html
keywords: nvme, linux, rhel, red hat, enterprise
summary: How to Configure NVMe-oF Host for RHEL 10.0 with ONTAP
---
= Configure RHEL 10.0 for NVMe-oF with ONTAP storage
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
include::_include/nvme/nvme-introduction.adoc[]

This document describes how to configure NVMe over Fabrics (NVMe-oF) hosts for RHEL 10.0. For more support and feature information, see link:hu-nvme-index.html[NVME-oF Overview^]. NVMe-oF with RHEL 10.0 has the following known limitations:

* DM multipath support for NVMe-oF is not supported.
* Avoid issuing the `nvme disconnect-all` command on systems booting from SAN over NVMe-TCP or NVMe-FC namespaces because it disconnects both root and data filesystems and might lead to system instability.

== Step 1: Optionally, enable SAN booting

include::_include/nvme/enable-san-booting.adoc[]

== Step 2: Verify the software version and NVMe configuration

include::_include/nvme/verify-software-version-introduction.adoc[]

.Steps

. Install RHEL 10.0 on the server. After the installation is complete, verify that you are running the required RHEL 10.0 kernel: 
+
[source,cli]
----
uname -r
----
+
Example RHEL kernel version:
+
----
6.12.0-55.9.1.el10_0.x86_64
----
. Install the `nvme-cli` package:
+
[source,cli]
----
rpm -qa|grep nvme-cli
----
+
The following example shows an `nvme-cli` package version:
+
----
nvme-cli-2.11-5.el10.x86_64
----

. Install the `libnvme` package:
+
[source,cli]
----
rpm -qa|grep libnvme
----
+
The following example shows an `libnvme` package version:
+
----
libnvme-1.11.1-1.el10.x86_64
----

. On the host, check the hostnqn string at `/etc/nvme/hostnqn`:
+
[source,cli]
----
cat /etc/nvme/hostnqn
----
+
The following example shows an `hostnqn` version:
+
----
nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
----

. Verify that the `hostnqn` string matches the `hostnqn` string for the corresponding subsystem on the ONTAP array:
+
[source,cli]
----
::> vserver nvme subsystem host show -vserver vs_nvme_194_rhel10
----
+
.Show example
[%collapsible]
====
----
Vserver Subsystem Priority  Host NQN
------- --------- --------  ------------------------------------------------
vs_ nvme_194_rhel10
        nvme4
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048- c7c04f425633
        nvme_1
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048- c7c04f425633
        nvme_2
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048- c7c04f425633
        nvme_3
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048- c7c04f425633
4 entries were displayed.
----
====

[NOTE]
If the `hostnqn` strings do not match, use the `vserver modify` command to update the `hostnqn` string on your corresponding ONTAP storage system subsystem to match the `hostnqn` string from `/etc/nvme/hostnqn` on the host.

== Step 3: Configure NVMe/FC and NVMe/TCP

Configure NVMe/FC with Broadcom/Emulex or Marvell/QLogic adapters, or configure NVMe/TCP using manual discovery and connect operations. 

[role="tabbed-block"]
=====
.FC - Broadcom/Emulex
--

Configure NVMe/FC for a Broadcom/Emulex adapter.

.Steps

. Verify that you are using the supported adapter model: 

.. Display the model names:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/modelname
----
+
You should see the following output:
+
----
LPe36002-M64 
LPe36002-M64
----

.. Display the model descriptions:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/modeldesc
----
+
You should see output similar to the following example:
+
----
Emulex LightPulse LPe36002-M64 2-Port 64Gb Fibre Channel Adapter 
Emulex LightPulse LPe36002-M64 2-Port 64Gb Fibre Channel Adapter
----

. Verify that you are using the recommended Broadcom `lpfc` firmware and inbox driver: 

.. Display the firmware version:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/fwrev 
----
+
The command returns the firmware versions:
+
----
14.0.539.16, sli-4:6:d
14.0.539.16, sli-4:6:d
----

.. Display the inbox driver version:
+
[source,cli]
----
cat /sys/module/lpfc/version
----
+
The following example shows a driver version:
+
----
0:14.4.0.6
----

+
For the current list of supported adapter driver and firmware versions, see the link:https://mysupport.netapp.com/matrix/[Interoperability Matrix Tool^].

. Verify that `lpfc_enable_fc4_type` is set to `3`:
+
[source,cli]
----
cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
----

. Verify that you can view your initiator ports:
+
[source,cli]
----
cat /sys/class/fc_host/host*/port_name
----
+
You should see output similar to:
+
----
0x100000109b1c1205
----

. Verify that your initiator ports are online:
+
[source,cli]
----
cat /sys/class/fc_host/host*/port_state
----
+
You should see the following output:
+
----
Online
Online
----

. Verify that the NVMe/FC initiator ports are enabled and that the target ports are visible:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/nvme_info
----
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
NVME Initiator Enabled
XRI Dist lpfc2 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc2 WWPN x100000109bf044b1 WWNN x200000109bf044b1 DID x022a00 *ONLINE*
NVME RPORT       WWPN x202fd039eaa7dfc8 WWNN x202cd039eaa7dfc8 DID x021310 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x202dd039eaa7dfc8 WWNN x202cd039eaa7dfc8 DID x020b10 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 0000000810 Cmpl 0000000810 Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000007b098f07 Issue 000000007aee27c4 OutIO ffffffffffe498bd
        abort 000013b4 noxri 00000000 nondlp 00000058 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 000013b4 Err 00021443

NVME Initiator Enabled
XRI Dist lpfc3 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc3 WWPN x100000109bf044b2 WWNN x200000109bf044b2 DID x021b00 *ONLINE*
NVME RPORT       WWPN x2033d039eaa7dfc8 WWNN x202cd039eaa7dfc8 DID x020110 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2032d039eaa7dfc8 WWNN x202cd039eaa7dfc8 DID x022910 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 0000000840 Cmpl 0000000840 Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000007afd4434 Issue 000000007ae31b83 OutIO ffffffffffe5d74f
        abort 000014a5 noxri 00000000 nondlp 0000006a qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 000014a5 Err 0002149a
----
====

--
.FC - Marvell/QLogic
--

Configure NVMe/FC for a Marvell/QLogic adapter.

.Steps

. Verify that you are using the supported adapter driver and firmware versions:
+
[source,cli]
----
cat /sys/class/fc_host/host*/symbolic_name
----
+
The following example shows driver and firmware versions:
+
----
QLE2872 FW:v9.15.00 DVR:v10.02.09.300-k
QLE2872 FW:v9.15.00 DVR:v10.02.09.300-k
----

. Verify that `ql2xnvmeenable` is set. This enables the Marvell adapter to function as an NVMe/FC initiator:
+
[source,cli]
----
cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
----
+
The expected output is 1.
--

.TCP
--

The NVMe/TCP protocol doesn't support the auto-connect operation. Instead, you can discover the NVMe/TCP subsystems and namespaces by performing the NVMe/TCP `connect` or `connect-all` operations manually.

.Steps

. Check that the initiator port can get the discovery log page data across the supported NVMe/TCP LIFs:
+
[source,cli]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme discover -t tcp -w 192.168.20.1 -a 192.168.20.20

Discovery Log Number of Records 8, Generation counter 18
=====Discovery Log Entry 0======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  4
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.64e65e6caae711ef9668d039ea951c46:discovery
traddr:  192.168.21.21
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 1======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  2
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.64e65e6caae711ef9668d039ea951c46:discovery
traddr:  192.168.20.21
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 2======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  3
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.64e65e6caae711ef9668d039ea951c46:discovery
traddr:  192.168.21.20
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 3======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  1
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.64e65e6caae711ef9668d039ea951c46:discovery
traddr:  192.168.20.20
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 4======
trtype:  tcp
adrfam:  ipv4
subtype: *nvme subsystem*
treq:    not specified
portid:  4
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.64e65e6caae711ef9668d039ea951c46:subsystem.rhel10_tcp_subsystem
traddr:  192.168.21.21
eflags:  none
sectype: none
=====Discovery Log Entry 5======
trtype:  tcp
adrfam:  ipv4
subtype: *nvme subsystem*
treq:    not specified
portid:  2
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.64e65e6caae711ef9668d039ea951c46:subsystem.rhel10_tcp_subsystem
traddr:  192.168.20.21
eflags:  none
sectype: none
=====Discovery Log Entry 6======
trtype:  tcp
adrfam:  ipv4
subtype: *nvme subsystem*
treq:    not specified
portid:  3
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.64e65e6caae711ef9668d039ea951c46:subsystem.rhel10_tcp_subsystem
traddr:  192.168.21.20
eflags:  none
sectype: none
=====Discovery Log Entry 7======
trtype:  tcp
adrfam:  ipv4
subtype: *nvme subsystem*
treq:    not specified
portid:  1
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.64e65e6caae711ef9668d039ea951c46:subsystem.rhel10_tcp_subsystem
traddr:  192.168.20.20
eflags:  none
sectype: none
----
====

. Verify that the other NVMe/TCP initiator-target LIF combinations can successfully retrieve discovery log page data: 
+
[source,cli]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme discover -t tcp -w 192.168.20.1 -a 192.168.20.20
nvme discover -t tcp -w 192.168.21.1 -a 192.168.21.20
nvme discover -t tcp -w 192.168.20.1 -a 192.168.20.21
nvme discover -t tcp -w 192.168.21.1 -a 192.168.21.21
----
====

. Run the `nvme connect-all` command across all the supported NVMe/TCP initiator-target LIFs across the nodes:
+
[source,cli]
----
nvme connect-all -t tcp -w host-traddr -a traddr
----
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme	connect-all	-t	tcp	-w	192.168.20.1	-a	192.168.20.20
nvme	connect-all	-t	tcp	-w	192.168.21.1	-a	192.168.21.20
nvme	connect-all	-t	tcp	-w	192.168.20.1	-a	192.168.20.21
nvme	connect-all	-t	tcp	-w	192.168.21.1	-a	192.168.21.21	
----			
====

[NOTE]
====
Beginning with RHEL 9.4, the setting for the NVMe/TCP `ctrl_loss_tmo timeout` is automatically set to "off". As a result:

include::_include/nvme/ctrl-loss-tmo-default-off.adoc[]
====
=====

== Step 4: Optionally, enable 1MB I/O for NVMe/FC

include::_include/nvme/nvme-enabling-broadcom-1mb-size.adoc[] 

== Step 5: Verify NVMe boot services 

include::_include/nvme/verify-nvme-services.adoc[]

== Step 6: Verify the multipathing configuration

include::_include/nvme/nvme-validate-nvme-of.adoc[]

. Verify that the controller state of each path is live and has the correct ANA status:
+ 
[role="tabbed-block"]
=====
.NVMe/FC
--
[source,cli]
----
nvme list-subsys /dev/nvme5n1
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme-subsys5 - NQN=nqn.1992-08.com.netapp:sn.f7565b15a66911ef9668d039ea951c46:subsystem.nvme1
               hostnqn=nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-c7c04f425633
\
 +- nvme126 *fc* traddr=nn-0x2036d039ea951c45:pn-0x2038d039ea951c45,host_traddr=nn-0x2000f4c7aa0cd7c3:pn-0x2100f4c7aa0cd7c3 *live optimized*
 +- nvme176 *fc* traddr=nn-0x2036d039ea951c45:pn-0x2037d039ea951c45,host_traddr=nn-0x2000f4c7aa0cd7c2:pn-0x2100f4c7aa0cd7c2 *live optimized*
 +- nvme5 *fc* traddr=nn-0x2036d039ea951c45:pn-0x2039d039ea951c45,host_traddr=nn-0x2000f4c7aa0cd7c2:pn-0x2100f4c7aa0cd7c2 *live non-optimized*
 +- nvme71 *fc* traddr=nn-0x2036d039ea951c45:pn-0x203ad039ea951c45,host_traddr=nn-0x2000f4c7aa0cd7c3:pn-0x2100f4c7aa0cd7c3 *live non-optimized*
----
====
--
.NVMe/TCP
--
[source,cli]
----
nvme list-subsys /dev/nvme4n2
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme-subsys4 - NQN=nqn.1992-08.com.netapp:sn.64e65e6caae711ef9668d039ea951c46:subsystem.nvme4
               hostnqn=nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-c2c04f444d33
\
+- nvme102 *tcp* traddr=192.168.21.20,trsvcid=4420,host_traddr=192.168.21.1,src_addr=192.168.21.1 *live non-optimized*
+- nvme151 *tcp* traddr=192.168.21.21,trsvcid=4420,host_traddr=192.168.21.1,src_addr=192.168.21.1 *live optimized*
+- nvme4 *tcp* traddr=192.168.20.20,trsvcid=4420,host_traddr=192.168.20.1,src_addr=192.168.20.1 *live non-optimized*
+- nvme53 *tcp* traddr=192.168.20.21,trsvcid=4420,host_traddr=192.168.20.1,src_addr=192.168.20.1 *live optimized*
----
====
--
=====

. Verify that the NetApp plug-in displays the correct values for each ONTAP namespace device:
+
[role="tabbed-block"]
=====
.Column
--
[source,cli]
----
nvme netapp ontapdevices -o column
----

.Show example
[%collapsible]
====
[subs=+quotes]
----

Device        Vserver   Namespace Path                                                          
----------------------- ------------------------------ 
/dev/nvme10n1     vs_tcp_rhel10       /vol/vol10/ns10 
              
NSID       UUID                                   Size
----------------------- ------------------------------
1    bbf51146-fc64-4197-b8cf-8a24f6f359b3   21.47GB                          
----
====
--
.JSON
--
[source,cli]
----
nvme netapp ontapdevices -o json
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
{
  "ONTAPdevices":[
    {
      "Device":"/dev/nvme10n1",
      "Vserver":"vs_tcp_rhel10",
      "Namespace_Path":"/vol/vol10/ns10",
      "NSID":1,
      "UUID":"bbf51146-fc64-4197-b8cf-8a24f6f359b3",
      "Size":"21.47GB",
      "LBA_Data_Size":4096,
      "Namespace_Size":5242880
}
]
    }
----
====
--
=====

== Step 7: Set up secure in-band authentication

Beginning with ONTAP 9.12.1, secure in-band authentication is supported over NVMe/TCP between a RHEL 10.0 host and an ONTAP controller.

Each host or controller must be associated with a `DH-HMAC-CHAP` key to set up secure authentication. A `DH-HMAC-CHAP` key is a combination of the NQN of the NVMe host or controller and an authentication secret configured by the administrator. To authenticate its peer, an NVMe host or controller must recognize the key associated with the peer. 

Set up secure in-band authentication using the CLI or a config JSON file. If you need to specify different dhchap keys for different subsystems, you must use a config JSON file. 

[role="tabbed-block"]
=====
.CLI
--
Set up secure in-band authentication using the CLI. 

.Steps
. Obtain the host NQN:
+
[source,cli]
----
cat /etc/nvme/hostnqn
----
. Generate the dhchap key for the RHEL 10.0 host.
+
The following output describes the `gen-dhchap-key` command parameters:
+
----
nvme gen-dhchap-key -s optional_secret -l key_length {32|48|64} -m HMAC_function {0|1|2|3} -n host_nqn 
•	-s secret key in hexadecimal characters to be used to initialize the host key
•	-l length of the resulting key in bytes
•	-m HMAC function to use for key transformation 
0 = none, 1- SHA-256, 2 = SHA-384, 3=SHA-512
•	-n host NQN to use for key transformation
----
+
In the following example, a random dhchap key with HMAC set to 3 (SHA-512) is generated.
+
----
nvme gen-dhchap-key -m 3 -n nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-c2c04f444d33
DHHC-1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:   
----
. On the ONTAP controller, add the host and specify both dhchap keys:
+
[source,cli]
----
vserver nvme subsystem host add -vserver <svm_name> -subsystem <subsystem> -host-nqn <host_nqn> -dhchap-host-secret <authentication_host_secret> -dhchap-controller-secret <authentication_controller_secret> -dhchap-hash-function {sha-256|sha-512} -dhchap-group {none|2048-bit|3072-bit|4096-bit|6144-bit|8192-bit}
----
. A host supports two types of authentication methods, unidirectional and bidirectional. On the host, connect to the ONTAP controller and specify dhchap keys based on the chosen authentication method:
+
[source,cli]
----
nvme connect -t tcp -w <host-traddr> -a <tr-addr> -n <host_nqn> -S <authentication_host_secret> -C <authentication_controller_secret>
----
. Validate the `nvme connect authentication` command by verifying the host and controller dhchap keys: 
+
.. Verify the host dhchap keys:
+
[source,cli]
----
cat /sys/class/nvme-subsystem/<nvme-subsysX>/nvme*/dhchap_secret
----
+
.Show example output for a unidirectional configuration
[%collapsible]
====
----
cat /sys/class/nvme-subsystem/nvme-subsys1/nvme*/dhchap_secret
DHHC- 1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jTmzEKUbcWu26I33b93b
il2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:  
DHHC- 1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jTmzEKUbcWu26I33b93b
il2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:  
DHHC- 1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jTmzEKUbcWu26I33b93b
il2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:  
DHHC- 1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jTmzEKUbcWu26I33b93b
il2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:  
----
====
+
.. Verify the controller dhchap keys:
+
[source,cli]
----
cat /sys/class/nvme-subsystem/<nvme-subsysX>/nvme*/dhchap_ctrl_secret
----
+
.Show example output for a bidirectional configuration
[%collapsible]
====
----
cat /sys/class/nvme-subsystem/nvme-subsys6/nvme*/dhchap_ctrl_secret
DHHC- 1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia
1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:
                      
DHHC- 1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia
1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:
                   
DHHC- 1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia
1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:

DHHC- 1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia
1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:
----
====
--
.JSON
--
When multiple NVMe subsystems are available on the ONTAP controller, you can use the `/etc/nvme/config.json` file with the `nvme connect-all` command. 

Use the `-o` option to generate the JSON file. Refer to the NVMe connect-all man pages for more syntax options.

.Steps
. Configure the JSON file.
+
[NOTE]
In the following example, `dhchap_key` corresponds to `dhchap_secret` and `dhchap_ctrl_key` corresponds to `dhchap_ctrl_secret`.
+
.Show example
[%collapsible]
====
----
cat /etc/nvme/config.json
[
{
"hostnqn":"nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-c2c04f444d33",
"hostid":"4c4c4544-0035-5910-804b-c2c04f444d33",
"dhchap_key":"DHHC-1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:",
"subsystems":[
{
"nqn":"nqn.1992-08.com.netapp:sn.127ade26168811f0a50ed039eab69ad3:subsystem.inband_unidirectional",
"ports":[
{
"transport":"tcp",
"traddr":"192.168.20.17",
"host_traddr":"192.168.20.1",
"trsvcid":"4420"
},
{
"transport":"tcp",
"traddr":"192.168.20.18",
"host_traddr":"192.168.20.1",
"trsvcid":"4420"
},
{
"transport":"tcp",
"traddr":"192.168.21.18",
"host_traddr":"192.168.21.1",
"trsvcid":"4420"
},
{
"transport":"tcp",
"traddr":"192.168.21.17",
"host_traddr":"192.168.21.1",
"trsvcid":"4420"
}]
----
==== 

. Connect to the ONTAP controller using the config JSON file:
+
[source,cli]
----
nvme connect-all -J /etc/nvme/config.json
----
+
.Show example
[%collapsible]
====
----
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
----
====

. Verify that the dhchap secrets have been enabled for the respective controllers for each subsystem.
+
.. Verify the host dhchap keys:
+
[source,cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys0/nvme0/dhchap_secret
----
+
The following example shows a dhchap key:
+
----
DHHC-1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia1
aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:
----
+
.. Verify the controller dhchap keys:
+
[source,cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys0/nvme0/dhchap_ctrl_secret
----
+
You should see output similar to the following example:
+
----
DHHC-1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jT
mzEKUbcWu26I33b93bil2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:
----
--
=====

== Step 8: Review the known issues

There are no known issues.

//ONTAPDOC-2561 25-Nov-2024
//ONTAPDOC-2987 13-May-2025