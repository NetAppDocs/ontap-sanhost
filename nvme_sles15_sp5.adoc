---
sidebar: sidebar
permalink: nvme_sles15_sp5.html
keywords: nvme, NVMe-oF, Host, SUSE Linux Enterprise Server 15 SP5, ONTAP
summary: NVMe-oF Host Configuration for SUSE Linux Enterprise Server 15 SP5 with ONTAP
---

= NVMe-oF Host Configuration for SUSE Linux Enterprise Server 15 SP5 with ONTAP
:toc: macro
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
NVMe over Fabrics (NVMe-oF), including NVMe over Fibre Channel (NVMe/FC) and other transports, is supported with SUSE Linux Enterprise Server (SLES) 15 SP5  with Asymmetric Namespace Access (ANA). In NVMe-oF environments, ANA is the equivalent of ALUA multipathing in iSCSI and FC environments and is implemented with in-kernel NVMe multipath. 

The following support is available for the NVMe-oF host configuration for SLES 15 SP5 with ONTAP:

* Support for NVMe over TCP (NVMe/TCP) in addition to NVMe/FC. 

* There is no sanlun support for NVMe-oF. Therefore, there is no host utility support for NVMe-oF on a SLES15 SP5 host. You can rely on the NetApp plug-in included in the native nvme-cli for the same instead. This should work for all NVMe-oF transports.

* Both NVMe and SCSI traffic can be run on the same co-existent host. In fact, that is expected to be the commonly deployed host configuration. Therefore, for SCSI, you might configure dm-multipath as usual for SCSI LUNs resulting in mpath devices, whereas NVMe multipath might be used to configure NVMe-oF multipath devices on the host.

For additional details on supported configurations, see the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix Tool^].

== Features 

* SLES 15 SP5 has in-kernel NVMe multipath enabled for NVMe namespaces by default, therefore, there is no need for explicit settings.

== Known limitations

There are no known limitations.
 
== Validate software versions

You can use the following procedure to validate the minimum supported SLES 15 SP5 software versions.

.Steps

. Verify that you have the requisite kernel and nvme-cli maintenance update (MU) packages installed on the SLES15 SP5 MU host. 
+
----
# uname -r
----
+
*Example output:*
+
----
5.14.21-150500.49-default
----

. Install the `nvme-cli` package:
+
----
# rpm -qa|grep nvme-cli
----
+
*Example output:*
+
----
nvme-cli-2.2.1+3.gd028407-150500.2.3.x86_64
----
+
The `nvme-cli` MU package now includes the following:

** *NVMe/FC auto-connect scripts* - The auto-connect scripts are required for NVMe/FC auto-reconnect when underlying paths to the namespaces are restored as well as during host reboot:
+
----
# rpm -ql nvme-cli-2.2.1+3.gd028407-150500.2.3.x86_64
/etc/nvme
/etc/nvme/discovery.conf
/etc/nvme/hostid
/etc/nvme/hostnqn
/usr/lib/systemd/system/nvmefc-boot-connections.service
/usr/lib/systemd/system/nvmf-autoconnect.service
/usr/lib/systemd/system/nvmf-connect.target
...

----

** *ONTAP udev rule* - New `udev` rule to verify that the NVMe multipath round-robin load balancer applies to all ONTAP namespaces by default:
+
----
# rpm -ql nvme-cli-1.13-3.3.1.x86_64
/etc/nvme
/etc/nvme/discovery.conf
/etc/nvme/hostid
/etc/nvme/hostnqn
/usr/lib/systemd/system/nvmefc-boot-connections.service
/usr/lib/systemd/system/nvmf-autoconnect.service
/usr/lib/systemd/system/nvmf-connect.target
/usr/lib/systemd/system/nvmf-connect@.service
/usr/lib/udev/rules.d/70-nvmf-autoconnect.rules
/usr/lib/udev/rules.d/71-nvmf-iopolicy-netapp.rules
...
# cat /usr/lib/udev/rules.d/71-nvmf-iopolicy-netapp.rules
# Enable round-robin for NetApp ONTAP and NetApp E-Series
ACTION=="add", SUBSYSTEM=="nvme-subsystem", ATTR{model}=="NetApp ONTAP Controller", ATTR{iopolicy}="round-robin"
ACTION=="add", SUBSYSTEM=="nvme-subsystem", ATTR{model}=="NetApp E-Series", ATTR{iopolicy}="round-robin"
----

** *NetApp plug-in for ONTAP devices* - The existing NetApp plug-in has now been modified to handle ONTAP namespaces as well.

. Install the libnvme package:
+
----
# rpm -qa|grep libnvme
----
+
*Example output:*
+
----
libnvme1-1.2+1.g41e1016-150500.2.5.x86_64
----

. Check the `hostnqn` string at `/etc/nvme/hostnqn`: 
+
----
# cat /etc/nvme/hostnqn
----
+
*Example output:*
+
----
nqn.2014-08.org.nvmexpress:uuid:ef34f6b7-74eb-1413-a64a-4c5262484877
----

. Verify that the `hostnqn` string matches the `hostnqn` string for the corresponding subsystem on the ONTAP array:
+
----
::> vserver nvme subsystem host show -vserver vs_nvme_43 
----
+
*Example output:*
+
----
Vserver     Subsystem          Host NQN
----------- --------------- ----------------------------------------------------------
vs_nvme_43   sles_nvme_43    nqn.2014-08.org.nvmexpress:uuid:ef34f6b7-74eb-1413-a64a-4c5262484877 
----
+
[NOTE]
If the `hostnqn` strings do not match, you can use the `vserver modify` command to update the `hostnqn` string on your corresponding ONTAP array subsystem to match the `hostnqn` string from `/etc/nvme/hostnqn` on the host.

== Configure NVMe/FC

You can configure NVMe/FC for Broadcom/Emulex adapters or Marvell/Qlogic adapters.

[role="tabbed-block"]
====
.Broadcom/Emulex
--
.Steps

. Verify that you are using the supported adapter model:
+
----
# cat /sys/class/scsi_host/host*/modelname
----
+
*Example output:*
+
----
LPe32002-M2
LPe32002-M2
----
+
----
# cat /sys/class/scsi_host/host*/modeldesc
----
+
*Example output:*
+
----
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
----

. Verify that you are using the recommended Broadcom `lpfc` firmware and inbox driver:
+
----
# cat /sys/class/scsi_host/host*/fwrev
14.2.455.11, sli-4:2:c
14.2.455.11, sli-4:2:c
----
+
The existing native inbox `lpfc` driver is the latest and compatible with NVMe/FC. Therefore, you do not need to install the lpfc out-of-box (OOB) driver. Verify the driver version:
+
----
# cat /sys/module/lpfc/version
0:14.2.0.9
----
+
The `lpfc nvme` support is already enabled by default. The newer lpfc drivers (both inbox and outbox) already have the `lpfc_enable_fc4_type` parameter set to 3, therefore, you no longer need to configure this explicitly in the `/etc/modprobe.d/lpfc.conf` file, and recreate `initrd`. You can verify that `lpfc_enable_fc4_type` is set to `3`  :
+
----
# cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
3
----
+
For the most current list of supported adapter driver and firmware versions, see the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix Tool^].


. Verify that the initiator ports are up and running, and that you can see the target LIFs:
+
----
# cat /sys/class/fc_host/host*/port_name
0x100000109b579d5e
0x100000109b579d5f

----
+
----

# cat /sys/class/fc_host/host*/port_state
Online
Online
----
+
----
# cat /sys/class/scsi_host/host*/nvme_info NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x10000090fac7fe48 WWNN x20000090fac7fe48 DID x022700 ONLINE
NVME RPORT	WWPN x209dd039ea16c28f WWNN x209cd039ea16c28f DID x020f0e TARGET DISCSRVC ONLINE

NVME Statistics
LS: Xmt 00000003e2 Cmpl 00000003e2 Abort 00000000
LS XMIT: Err 00000000	CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 00000000000f36cd Issue 00000000000f36ce OutIO 0000000000000001
abort 00000000 noxri 00000000 nondlp 00000000 qdepth 00000000
wqerr 00000000 err 00000000
FCP CMPL: xb 000000bc Err 000001d8

NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x10000090fac7fe49 WWNN x20000090fac7fe49 DID x022d00 ONLINE
NVME RPORT	WWPN x20a0d039ea16c28f WWNN x209cd039ea16c28f DID x02010f TARGET DISCSRVC ONLINE
NVME RPORT	WWPN x209ed039ea16c28f WWNN x209cd039ea16c28f DID x020d0f TARGET DISCSRVC ONLINE

NVME Statistics
LS: Xmt 000000056a Cmpl 000000056a Abort 00000000
LS XMIT: Err 00000000	CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000000010af3e Issue 000000000010af40 OutIO 0000000000000002
abort 00000000 noxri 00000000 nondlp 00000000 qdepth 00000000
wqerr 00000000 err 00000000
FCP CMPL: xb 00000102 Err 0000028e 3

----
--

.Marvell/QLogic FC Adapter for NVMe/FC
--
.Steps

.  The native inbox qla2xxx driver included in the SLES 15 SP5 kernel has the latest upstream fixes essential for ONTAP support. Verify that you are running the supported adapter driver and firmware versions:
+
----
# cat /sys/class/fc_host/host*/symbolic_name
QLE2742 FW:v9.08.02 DVR:v10.02.07.800-k 
QLE2742 FW:v9.08.02 DVR:v10.02.07.800-k
----

. Verify that `ql2xnvmeenable` is set. This enables the Marvell adapter to function as an NVMe/FC initiator:
+
----
# cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
1
----
--
====

=== Enable 1MB I/O size (Optional)

include::_include/nvme/reuse_nvme_enabling_broadcom_1mb_size.adoc[]

== Configure NVMe/TCP

include::_include/nvme/reuse_configure_nvmetcp.adoc[]

.Steps

. Verify that the initiator port can fetch the discovery log page data across the supported NVMe/TCP LIFs:
+
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
*Example output:*
+
----
# nvme discover -t tcp -w 192.168.6.5 -a 192.168.6.35 Discovery Log Number of Records 8, Generation counter 18
=====Discovery Log Entry 0======
trtype:  tcp
adrfam:  ipv4
subtype: current discovery subsystem
treq:    not specified
portid:  0
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.f7f9730b664711eda32dd039ea16c290:discovery
traddr:  192.168.7.35
eflags:  explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 1======
trtype:  tcp
adrfam:  ipv4
subtype: current discovery subsystem
treq:    not specified
portid:  1
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.f7f9730b664711eda32dd039ea16c290:discovery
traddr:  192.168.7.34
eflags:  explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 2======
trtype:  tcp
adrfam:  ipv4
subtype: current discovery subsystem
treq:    not specified
portid:  2
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.f7f9730b664711eda32dd039ea16c290:discovery
traddr:  192.168.6.35
eflags:  explicit discovery connections, duplicate discovery information
sectype: none
...
..........

----

. Verify that the other NVMe/TCP initiator-target LIF combinations can successfully fetch discovery log page data: 
+
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
*Example output:*
+
----
# nvme discover -t tcp -w 192.168.6.5 -a 192.168.6.34
# nvme discover -t tcp -w 192.168.6.5 -a 192.168.6.35
# nvme discover -t tcp -w 192.168.7.5 -a 192.168.7.34 
# nvme discover -t tcp -w 192.168.7.5 -a 192.168.7.35
----

. Run the `nvme connect-all` command across all the supported NVMe/TCP initiator-target LIFs across the nodes, and set the controller loss timeout period for at least 30 minutes or 1800 seconds:
+
----
nvme connect-all -t tcp -w host-traddr -a traddr -l 1800
----
+
*Example output:*
+
----
#nvme	connect-all -t	tcp -w	192.168.6.5 -a	192.168.6.34	-l	1800
#nvme	connect-all -t	tcp -w	192.168.6.5 -a	192.168.6.35	-l	1800
#nvme	connect-all -t	tcp -w	192.168.7.5 -a	192.168.7.34	-l	1800
#nvme	connect-all -t	tcp -w	192.168.7.5 -a	192.168.7.35	-l	1800

----


== Validate NVMe-oF

You can use the following procedure to validate NVMe-oF.

.Steps

. Verify the following NVMe/FC settings on the SLES 15 SP5 host:
+
----
# cat /sys/module/nvme_core/parameters/multipath
Y
----
+
----
# cat /sys/class/nvme-subsystem/nvme-subsys*/model
NetApp ONTAP Controller
NetApp ONTAP Controller
----
+
----
# cat /sys/class/nvme-subsystem/nvme-subsys*/iopolicy
round-robin
round-robin
----

. Verify that the namespaces are created and correctly discovered on the host:
+
----
# nvme list
----
+
*Example output:*
+
----
Node         SN                   Model                          
---------------------------------------------------------  
/dev/nvme0n1 81CZ5BQuUNfGAAAAAAAB  NetApp ONTAP Controller
                                  


Namespace Usage    Format             FW             Rev
-----------------------------------------------------------
1                 85.90 GB / 85.90 GB	4 KiB + 0 B  FFFFFFFF
     
----

. Verify that the controller state of each path is live and has the correct ANA status:
+
[role="tabbed-block"]
====
.NVMe/FC
--
----
# nvme list-subsys /dev/nvme1n1
----

*Example output:*

----
nvme-subsys1 - NQN=nqn.1992-
08.com.netapp:sn.04ba0732530911ea8e8300a098dfdd91:subsystem.nvme_145_1
\
+- nvme2 fc traddr=nn-0x208100a098dfdd91:pn-0x208200a098dfdd91 host_traddr=nn-0x200000109b579d5f:pn-0x100000109b579d5f live non- optimized
+- nvme3 fc traddr=nn-0x208100a098dfdd91:pn-0x208500a098dfdd91 host_traddr=nn-0x200000109b579d5e:pn-0x100000109b579d5e live non- optimized
+- nvme4 fc traddr=nn-0x208100a098dfdd91:pn-0x208400a098dfdd91 host_traddr=nn-0x200000109b579d5e:pn-0x100000109b579d5e live optimized
+- nvme6 fc traddr=nn-0x208100a098dfdd91:pn-0x208300a098dfdd91 host_traddr=nn-0x200000109b579d5f:pn-0x100000109b579d5f live optimized
----
--

.NVMe/TCP
--
----
nvme list-subsys /dev/nvme1n1 
----
*Example output*
----
nvme-subsys1 - NQN=nqn.1992-08.com.netapp:sn.f7f9730b664711eda32dd039ea16c290:subsystem.tcpnvme_sles15sp5
\
 +- nvme5 tcp traddr=192.168.7.34,trsvcid=4420,host_traddr=192.168.7.5 live
 +- nvme4 tcp traddr=192.168.7.35,trsvcid=4420,host_traddr=192.168.7.5 live
 +- nvme3 tcp traddr=192.168.6.34,trsvcid=4420,host_traddr=192.168.6.5 live
 +- nvme2 tcp traddr=192.168.6.35,trsvcid=4420,host_traddr=192.168.6.5 live

----
--
====

. Verify that the NetApp plug-in displays the correct values for each ONTAP namespace device:
+
[role="tabbed-block"]
====
.Column
--
----
# nvme netapp ontapdevices -o column
----

*Example output:*

----
Device        Vserver   Namespace Path                                                          
----------------------- ------------------------------ 
/dev/nvme1n11   vs_tcp_129   /vol/tcpnvme_129_1/ns1
          


NSID       UUID                                   Size
------------------------------------------------------------
1          a6aee036-e12f-4b07-8e79-4d38a9165686   32.90GB

----
--
.JSON
--
----
# nvme netapp ontapdevices -o json
----
*Example output*
----
{
"ONTAPdevices" : [
{
"Device":"/dev/nvme1n11",
      "Vserver":"vs_tcp_129",
      "Namespace_Path":"/vol/tcpnvme_129_1/ns1",
      "NSID":1,
      "UUID":"919c602d-f080-4dd8-8b15-e83e6f247714",
      "Size":"32.21GB",
      "LBA_Data_Size":4096,
      "Namespace_Size":7864320 
}
]

}

----
--
====

== Known issues

There are no known issues.


== Troubleshooting

Before troubleshooting any NVMe-oF failures, verify that you are running a configuration that is compliant to the IMT specifications and then proceed with the next steps to debug any host side issues.

=== Enable verbose logging

If you have an issue with your configuration, verbose logging can provide essential information for troubleshooting.

.Steps

The procedure to set verbose logging for Qlogic (Qla2xxx) is different from the procedure to set LPFC verbose logging.

[role="tabbed-block"]
====

.LPFC
--

include::_include/nvme/reuse_nvme_verbose_logging.adoc[]
--

.Qla2xxx 
--
include::_include/nvme/reuse_nvme_qla2xxx_verbose_logging.adoc[]
--
====

include::_include/hu/reuse_hu_common_nvme_cli_errors.adoc[]

=== When to contact technical support

include::_include/hu/reuse_hu_contact_tech_support.adoc[]







