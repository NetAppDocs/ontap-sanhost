---
sidebar: sidebar
permalink: nvme-rhel-93.html
keywords: nvme, linux, rhel, red hat, enterprise
summary: How to Configure NVMe-oF Host for RHEL 9.3 with ONTAP
---
= Configure RHEL 9.3 for NVMe-oF with ONTAP storage
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
include::_include/nvme/nvme-introduction.adoc[] 

This document describes how to configure NVMe over Fabrics (NVMe-oF) hosts for RHEL 9.3. For more support and feature information, see link:hu-nvme-index.html[NVME-oF Overview^]. NVMe-oF with RHEL 9.3 has the following known limitations:

* SAN booting using the NVMe-oF protocol is not currently supported.

== Step 1: Optionally, enable SAN booting

include::_include/nvme/enable-san-booting.adoc[]

== Step 2: Verify the software version and NVMe configuration

include::_include/nvme/verify-software-version-introduction.adoc[]

.Steps

. Install RHEL 9.3 on the server. After the installation is complete, verify that you are running the required RHEL 9.3 kernel: 
+
[source,cli]
----
uname -r
----
+
Example RHEL kernel version:
+
----
5.14.0-362.8.1.el9_3.x86_64
----

. Install the `nvme-cli` package:
+
[source,cli]
----
rpm -qa|grep nvme-cli
----
+
The following example shows an nvme-cli package version:
+
----
nvme-cli-2.4-10.el9.x86_64
----

. Install the `libnvme` package:
+
[source,cli]
----
rpm -qa|grep libnvme
----
+
The following example shows an libnvme package version:
+
----
libnvme-1.4-7.el9.x86_64
----

. On the host, check the hostnqn string at `/etc/nvme/hostnqn`:
+
[source,cli]
----
cat /etc/nvme/hostnqn
----
+
The following example shows an `hostnqn` version:
+
----
nqn.2014-08.org.nvmexpress:uuid:060fd513-83be-4c3e-aba1-52e169056dcf
----

. Verify that the `hostnqn` string matches the `hostnqn` string for the corresponding subsystem on the ONTAP array:
+
[source,cli]
----
::> vserver nvme subsystem host show -vserver vs_nvme147
----
+
.Show example
[%collapsible]
====
----
Vserver     Subsystem          Host NQN
----------- --------------- ----------------------------------------------------------
vs_nvme147   rhel_147_LPe32002    nqn.2014-08.org.nvmexpress:uuid:060fd513-83be-4c3e-aba1-52e169056dcf
----
====

[NOTE]
If the `hostnqn` strings do not match, use the `vserver modify` command to update the `hostnqn` string on your corresponding ONTAP storage system subsystem to match the `hostnqn` string from `/etc/nvme/hostnqn` on the host.

== Step 3: Configure NVMe/FC and NVMe/TCP

Configure NVMe/FC with Broadcom/Emulex or Marvell/QLogic adapters, or configure NVMe/TCP using manual discovery and connect operations. 

[role="tabbed-block"]
=====
.FC - Broadcom/Emulex
--

Configure NVMe/FC for a Broadcom/Emulex adapter.

.Steps

. Verify that you are using the supported adapter model:

.. Display the model names:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/modelname
----
+
You should see the following output:
+
----
LPe32002-M2
LPe32002-M2
----
        
.. Display the model descriptions:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/modeldesc
----
+
You should see output similar to the following example:
+
----
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
----
        
. Verify that you are using the recommended Broadcom `lpfc` firmware and inbox driver:
        
.. Display the firmware version:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/fwrev
----
+
The command returns firmware versions:
+
----
14.2.539.16, sli-4:2:c
14.2.539.16, sli-4:2:c
----
        
.. Display the inbox driver version:
+
[source,cli]
----
cat /sys/module/lpfc/version
----
+
The following example shows a driver version:
+
----
0:14.2.0.12
----
+        
For the current list of supported adapter driver and firmware versions, see the link:https://mysupport.netapp.com/matrix/[Interoperability Matrix Tool^].
        
. Verify that `lpfc_enable_fc4_type` is set to `3`:
+
[source,cli]
----
cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
----
        
. Verify that you can view your initiator ports:
+
[source,cli]
----
cat /sys/class/fc_host/host*/port_name
----
+
You should see output similar to:
+
----
0x100000109b3c081f
0x100000109b3c0820
----
        
. Verify that your initiator ports are online:
+
[source,cli]
----
cat /sys/class/fc_host/host*/port_state
----
+
You should see the following output:
+
----
Online
Online
----
        
. Verify that the NVMe/FC initiator ports are enabled and that the target ports are visible:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/nvme_info
----
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x100000109b3c081f WWNN x200000109b3c081f DID x062300 *ONLINE*
NVME RPORT       WWPN x2143d039ea165877 WWNN x2142d039ea165877 DID x061b15 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2145d039ea165877 WWNN x2142d039ea165877 DID x061115 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 000000040b Cmpl 000000040b Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000001f5c4538 Issue 000000001f58da22 OutIO fffffffffffc94ea
abort 00000630 noxri 00000000 nondlp 00001071 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00000630 Err 0001bd4a

NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x100000109b3c0820 WWNN x200000109b3c0820 DID x062c00 *ONLINE*
NVME RPORT       WWPN x2144d039ea165877 WWNN x2142d039ea165877 DID x060215 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2146d039ea165877 WWNN x2142d039ea165877 DID x061815 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 000000040b Cmpl 000000040b Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000001f5c3618 Issue 000000001f5967a4 OutIO fffffffffffd318c
abort 00000629 noxri 00000000 nondlp 0000044e qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00000629 Err 0001bd3d
----
====

--
.FC - Marvell/QLogic
--

Configure NVMe/FC for a Marvell/QLogic adapter.

.Steps

. Verify that you are using the supported adapter driver and firmware versions:
+
[source,cli]
----
cat /sys/class/fc_host/host*/symbolic_name
----
+
The following example shows driver and firmware versions:
+
----
QLE2772 FW:v9.10.11 DVR:v10.02.08.200-k
QLE2772 FW:v9.10.11 DVR:v10.02.08.200-k
----

. Verify that `ql2xnvmeenable` is set. This enables the Marvell adapter to function as an NVMe/FC initiator:
+
[source,cli]
----
cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
----
+
The expected output is 1.
--

.TCP
--

The NVMe/TCP protocol does not support the auto-connect operation. You must manually perform the NVMe/TCP connect or connect-all operations to discover the NVMe/TCP subsystems and namespaces.

.Steps

. Check that the initiator port can get the discovery log page data across the supported NVMe/TCP LIFs:
+
[source,cli]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme discover -t tcp -w 192.168.167.1 -a 192.168.167.16

Discovery Log Number of Records 8, Generation counter 10
=====Discovery Log Entry 0======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  0
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.bbfb4ee8dfb611edbd07d039ea165590:discovery
traddr:  192.168.166.17
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 1======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  1
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.bbfb4ee8dfb611edbd07d039ea165590:discovery
traddr:  192.168.167.17
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 2======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  2
trsvcid: 8009
subnqn:  nqn.1992-
08.com.netapp:sn.bbfb4ee8dfb611edbd07d039ea165590:discovery
traddr:  192.168.166.16
eflags: *explicit discovery connections, duplicate discovery information*
sectype: none
=====Discovery Log Entry 3======
trtype:  tcp
adrfam:  ipv4
subtype: *current discovery subsystem*
treq:    not specified
portid:  3
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.bbfb4ee8dfb611edbd07d039ea165590:discovery
traddr:  192.168.167.16
eflags:  *explicit discovery connections, duplicate discovery information*
sectype: none
----
====

. Verify that the other NVMe/TCP initiator-target LIF combinations can successfully retrieve discovery log page data: 
+
[source,cli]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme discover -t tcp -w 192.168.166.5 -a 192.168.166.22
nvme discover -t tcp -w 192.168.166.5 -a 192.168.166.23 
nvme discover -t tcp -w 192.168.167.5 -a 192.168.167.22
nvme discover -t tcp -w 192.168.167.5 -a 192.168.167.23
----
====

. Run the `nvme connect-all` command across all the supported NVMe/TCP initiator-target LIFs across the nodes:
+
[source,cli]
----
nvme connect-all -t tcp -w host-traddr -a traddr
----
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme	connect-all	-t	tcp	-w	192.168.166.1	-a	192.168.166.16 -l	1800							
nvme	connect-all	-t	tcp	-w	192.168.166.1	-a	192.168.166.17 -l	1800							
nvme	connect-all	-t	tcp	-w	192.168.167.1	-a	192.168.167.16 -l	1800							
nvme	connect-all	-t	tcp	-w	192.168.167.1	-a	192.168.167.17 -l	1800							
----			
====

--
=====

== Step 4: Optionally, enable 1MB I/O for NVMe/FC

include::_include/nvme/nvme-enabling-broadcom-1mb-size.adoc[] 

== Step 5: Verify the multipathing configuration

include::_include/nvme/nvme-validate-nvme-of.adoc[]

. Verify that the controller state of each path is live and has the correct ANA status:
+
[role="tabbed-block"]
=====
.NVMe/FC
--
[source,cli]
----
nvme list-subsys /dev/nvme4n5
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme-subsys4 - NQN=nqn.1992-08.com.netapp:sn.e80cc121ca6911ed8cbdd039ea165590:subsystem.rhel_
147_LPE32002
\
 +- nvme2 *fc* traddr=nn-0x2142d039ea165877:pn-0x2144d039ea165877,host_traddr=nn-0x200000109b3c0820:pn-0x100000109b3c0820 *live optimized*
 +- nvme3 *fc* traddr=nn-0x2142d039ea165877:pn-0x2145d039ea165877,host_traddr=nn-0x200000109b3c081f:pn-0x100000109b3c081f *live non-optimized*
 +- nvme4 *fc* traddr=nn-0x2142d039ea165877:pn-0x2146d039ea165877,host_traddr=nn-0x200000109b3c0820:pn-0x100000109b3c0820 *live non-optimized*
 +- nvme6 *fc* traddr=nn-0x2142d039ea165877:pn-0x2143d039ea165877,host_traddr=nn-0x200000109b3c081f:pn-0x100000109b3c081f *live optimized*
----
====
--
.NVMe/TCP
--
[source,cli]
----
nvme list-subsys /dev/nvme1n1 
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme-subsys1 - NQN=nqn.1992- 08.com.netapp:sn. bbfb4ee8dfb611edbd07d039ea165590:subsystem.rhel_tcp_95
+- nvme1 *tcp* traddr=192.168.167.16,trsvcid=4420,host_traddr=192.168.167.1,src_addr=192.168.167.1 *live*
+- nvme2 *tcp* traddr=192.168.167.17,trsvcid=4420,host_traddr=192.168.167.1,src_addr=192.168.167.1 *live*
+- nvme3 *tcp* traddr=192.168.167.17,trsvcid=4420,host_traddr=192.168.166.1,src_addr=192.168.166.1 *live* 
+- nvme4 *tcp* traddr=192.168.166.16,trsvcid=4420,host_traddr=192.168.166.1,src_addr=192.168.166.1 *live*
----
====
--
=====

. Verify that the NetApp plug-in displays the correct values for each ONTAP namespace device:
+
[role="tabbed-block"]
=====
.Column
--
[source,cli]
----
nvme netapp ontapdevices -o column
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
Device        Vserver   Namespace Path                                                          
----------------------- ------------------------------ 
/dev/nvme0n1 vs_tcp           /vol/vol1/ns1   
              


NSID       UUID                                   Size
------------------------------------------------------------
1          6fcb8ea0-dc1e-4933-b798-8a62a626cb7f	21.47GB
----
====
--
.JSON
--
[source,cli]
----
nvme netapp ontapdevices -o json
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
{

"ONTAPdevices" : [
{

"Device" : "/dev/nvme1n1", 
"Vserver" : "vs_tcp_95", 
"Namespace_Path" : "/vol/vol1/ns1", 
"NSID" : 1,
"UUID" : "6fcb8ea0-dc1e-4933-b798-8a62a626cb7f", 
"Size" : "21.47GB",
"LBA_Data_Size" : 4096,
"Namespace_Size" : 5242880
},

]
}
----
====
--
=====

== Step 6: Set up secure in-band authentication

Beginning with ONTAP 9.12.1, secure in-band authentication is supported over NVMe/TCP between a RHEL 9.3 host and an ONTAP controller.

Each host or controller must be associated with a `DH-HMAC-CHAP` key to set up secure authentication. A `DH-HMAC-CHAP` key is a combination of the NQN of the NVMe host or controller and an authentication secret configured by the administrator. To authenticate its peer, an NVMe host or controller must recognize the key associated with the peer. 

Set up secure in-band authentication using the CLI or a config JSON file. If you need to specify different dhchap keys for different subsystems, you must use a config JSON file. 

[role="tabbed-block"]
=====
.CLI
--
Set up secure in-band authentication using the CLI. 

.Steps
. Obtain the host NQN:
+
[source,cli]
----
cat /etc/nvme/hostnqn
----
. Generate the dhchap key for the RHEL 9.3 host.
+
The following output describes the `gen-dhchap-key` command parameters:
+
----
nvme gen-dhchap-key -s optional_secret -l key_length {32|48|64} -m HMAC_function {0|1|2|3} -n host_nqn 
•	-s secret key in hexadecimal characters to be used to initialize the host key
•	-l length of the resulting key in bytes
•	-m HMAC function to use for key transformation 
0 = none, 1- SHA-256, 2 = SHA-384, 3=SHA-512
•	-n host NQN to use for key transformation
----
+
In the following example, a random dhchap key with HMAC set to 3 (SHA-512) is generated.
+
----
nvme gen-dhchap-key -m 3 -n nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-c2c04f444d33
DHHC-1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:   
----
. On the ONTAP controller, add the host and specify both dhchap keys:
+
[source,cli]
----
vserver nvme subsystem host add -vserver <svm_name> -subsystem <subsystem> -host-nqn <host_nqn> -dhchap-host-secret <authentication_host_secret> -dhchap-controller-secret <authentication_controller_secret> -dhchap-hash-function {sha-256|sha-512} -dhchap-group {none|2048-bit|3072-bit|4096-bit|6144-bit|8192-bit}
----
. A host supports two types of authentication methods, unidirectional and bidirectional. On the host, connect to the ONTAP controller and specify dhchap keys based on the chosen authentication method:
+
[source,cli]
----
nvme connect -t tcp -w <host-traddr> -a <tr-addr> -n <host_nqn> -S <authentication_host_secret> -C <authentication_controller_secret>
----
. Validate the `nvme connect authentication` command by verifying the host and controller dhchap keys: 
+
.. Verify the host dhchap keys:
+
[source,cli]
----
cat /sys/class/nvme-subsystem/<nvme-subsysX>/nvme*/dhchap_secret
----
+
.Show example output for a unidirectional configuration
[%collapsible]
====
----
cat /sys/class/nvme-subsystem/nvme-subsys1/nvme*/dhchap_secret
DHHC- 1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jTmzEKUbcWu26I33b93b
il2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:  
DHHC- 1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jTmzEKUbcWu26I33b93b
il2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:  
DHHC- 1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jTmzEKUbcWu26I33b93b
il2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:  
DHHC- 1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jTmzEKUbcWu26I33b93b
il2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:  
----
====
+
.. Verify the controller dhchap keys:
+
[source,cli]
----
cat /sys/class/nvme-subsystem/<nvme-subsysX>/nvme*/dhchap_ctrl_secret
----
+
.Show example output for a bidirectional configuration
[%collapsible]
====
----
cat /sys/class/nvme-subsystem/nvme-subsys6/nvme*/dhchap_ctrl_secret
DHHC- 1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia
1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:
                      
DHHC- 1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia
1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:
                   
DHHC- 1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia
1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:

DHHC- 1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia
1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:
----
====
--
.JSON
--
When multiple NVMe subsystems are available on the ONTAP controller, you can use the `/etc/nvme/config.json` file with the `nvme connect-all` command. 

Use the `-o` option to generate the JSON file. Refer to the NVMe connect-all man pages for more syntax options.

.Steps
. Configure the JSON file.
+
[NOTE]
In the following example, `dhchap_key` corresponds to `dhchap_secret` and `dhchap_ctrl_key` corresponds to `dhchap_ctrl_secret`.
+
.Show example
[%collapsible]
====
----
cat /etc/nvme/config.json
[
{
"hostnqn":"nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0035-5910-804b-c2c04f444d33",
"hostid":"4c4c4544-0035-5910-804b-c2c04f444d33",
"dhchap_key":"DHHC-1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia1aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:",
"subsystems":[
{
"nqn":"nqn.1992-08.com.netapp:sn.127ade26168811f0a50ed039eab69ad3:subsystem.inband_unidirectional",
"ports":[
{
"transport":"tcp",
"traddr":"192.168.20.17",
"host_traddr":"192.168.20.1",
"trsvcid":"4420"
},
{
"transport":"tcp",
"traddr":"192.168.20.18",
"host_traddr":"192.168.20.1",
"trsvcid":"4420"
},
{
"transport":"tcp",
"traddr":"192.168.21.18",
"host_traddr":"192.168.21.1",
"trsvcid":"4420"
},
{
"transport":"tcp",
"traddr":"192.168.21.17",
"host_traddr":"192.168.21.1",
"trsvcid":"4420"
}]
----
==== 

. Connect to the ONTAP controller using the config JSON file:
+
[source,cli]
----
nvme connect-all -J /etc/nvme/config.json
----
+
.Show example
[%collapsible]
====
----
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.20 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
traddr=192.168.20.21 is already connected
----
====

. Verify that the dhchap secrets have been enabled for the respective controllers for each subsystem.
+
.. Verify the host dhchap keys:
+
[source,cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys0/nvme0/dhchap_secret
----
+
The following example shows a dhchap key:
+
----
DHHC-1:03:7zf8I9gaRcDWH3tCH5vLGaoyjzPIvwNWusBfKdpJa+hia1
aKDKJQ2o53pX3wYM9xdv5DtKNNhJInZ7X8wU2RQpQIngc=:
----
+
.. Verify the controller dhchap keys:
+
[source,cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys0/nvme0/dhchap_ctrl_secret
----
+
You should see output similar to the following example:
+
----
DHHC-1:03:fMCrJharXUOqRoIsOEaG6m2PH1yYvu5+z3jT
mzEKUbcWu26I33b93bil2WR09XDho/ld3L45J+0FeCsStBEAfhYgkQU=:
----
--
=====

== Step 7: Review the known issues

There are no known issues.


// 2022,12-06, Jira IEOPS-690

// JIRA-1289 20-Sep-2023
