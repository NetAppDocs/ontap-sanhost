---
sidebar: sidebar
permalink: nvme_esxi_8.html
keywords: nvme, esxi, ontap, nvme/fc, hypervisor
summary: Describes how to configure NVMe-oF for ESXi 7.x with ONTAP
---

= NVMe-oF Host Configuration for ESXi 8.x with ONTAP
:toc: macro
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

== Supportability

* Beginning with ONTAP 9.7, NVMe over Fibre Channel (NVMe/FC) support is added for VMware vSphere releases.
* NVMe/TCP feature is supported for ESXi Hypervisor in ESXi 8.0 and later.
* Beginning with ONTAP 9.10.1, NVMe/TCP feature is supported for ONTAP.


== Features

*	ESXi initiator host can run both NVMe/FC and FCP traffic through the same adapter ports. See the link:https://hwu.netapp.com/Home/Index[Hardware Universe^] for a list of supported FC adapters and controllers. See the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix^] for the most current list of supported configurations and versions.
*	Beginning with ONTAP 9.9.1 P3, NVMe/FC feature is supported for ESXi 7.0 update 3.
*	For ESXi 7.0 and later releases, HPP (high performance plugin) is the default plugin for NVMe devices.

==	Known limitations

The following configurations are not supported:

* RDM mapping
* VVols


== Enable NVMe/FC

You need to check the ESXi host NQN string and verify that it matches with the host NQN string for the corresponding subsystem on the ONTAP array. Use the following command:
+
----
# esxcli nvme  info get
Host NQN: nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436

# vserver nvme subsystem host show -vserver nvme_fc
Vserver Subsystem Host NQN
------- --------- ----------------------------------------------------------
nvme_fc nvme_ss  nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----

=== Configure Broadcom/Emulex

.Steps

. Check whether the configuration is supported with required driver/firmware by referring to link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix Tool^].

. Set the `lpfc` driver parameter `lpfc_enable_fc4_type=3` for enabling NVMe/FC support in the `lpfc` driver and reboot the host.

[NOTE] 
====
* Starting with vSphere 7.0 update 3, the `brcmnvmefc` driver is no longer available.  Therefore, the `lpfc` driver now includes the NVMe over Fibre Channel (NVMe/FC) functionality previously delivered with the `brcmnvmefc` driver.

* The `lpfc_enable_fc4_type=3` parameter is set by default for the LPe35000-series adapters. You need to perform the following command to set it manually for LPe32000-series and LPe31000-series adapters.
+
----
# esxcli system module parameters set -m lpfc -p lpfc_enable_fc4_type=3
# esxcli system module parameters list  -m lpfc | grep lpfc_enable_fc4_type

lpfc_enable_fc4_type              int            Defines what FC4 types are supported
# esxcli storage core adapter list

HBA Name  Driver      Link State  UID                                   
--------  ----------  ----------  ------------------------------------  

vmhba0    vmw_ahci    link-n/a    sata.vmhba0       
vmhba1    vmw_ahci    link-n/a    sata.vmhba1        
vmhba2    lsi_mr3     link-n/a    sas.5003005702854d00       
vmhba3    qlnativefc  link-up     fc.20000024ff7f4a50:21000024ff7f4a50  
vmhba4    qlnativefc  link-up     fc.20000024ff7f4a51:21000024ff7f4a51  
vmhba64   qlnativefc  link-up     fc.20000024ff7f4a50:21000024ff7f4a50                       
vmhba65   qlnativefc  link-up     fc.20000024ff7f4a51:21000024ff7f4a51 

Capabilities         Description
-------------------------------------
(0000:00:11.4) Intel Corporation Wellsburg AHCI Controller
(0000:00:1f.2) Intel Corporation Wellsburg AHCI Controller
(0000:01:00.0) Broadcom MegaRAID SAS Fury Controller
Second Level Lun ID  (0000:81:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter
Second Level Lun ID  (0000:81:00.1) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter
(0000:81:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter
(0000:81:00.1) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter

----
====

=== Configure Marvell/QLogic

.Steps

. Check whether configuration is supported with required driver/firmware by referring to link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix Tool^].

. Set the `qlnativefc` driver parameter `ql2xnvmesupport=1` to enable NVMe/FC support in the `qlnativefc` driver and reboot the host.
+
`# esxcfg-module -s 'ql2xnvmesupport=1' qlnativefc`
+

[NOTE] 
====
The `qlnativefc` driver parameter is set by default for the Qle 277x-series adapters. You must perform the following command to set it manually for Qle 277x series adapters.

----
esxcfg-module -l | grep qlnativefc
qlnativefc               4    1820
----
====

. Check whether nvme is enabled on the adapter:
+
----
# esxcli storage core adapter list
HBA Name  Driver      Link State  UID                                   
--------  ----------  ----------  ------------------------------------  
vmhba0    vmw_ahci    link-n/a    sata.vmhba0                                                
vmhba1    vmw_ahci    link-n/a    sata.vmhba1                                                
vmhba2    lsi_mr3     link-n/a    sas.5003005702854d00                                       
vmhba3    qlnativefc  link-up     fc.20000024ff7f4a50:21000024ff7f4a50  
vmhba4    qlnativefc  link-up     fc.20000024ff7f4a51:21000024ff7f4a51  
vmhba64   qlnativefc  link-up     fc.20000024ff7f4a50:21000024ff7f4a50                       

Capabilities         Description
--------------------------------------
(0000:00:11.4) Intel Corporation Wellsburg AHCI Controller
(0000:00:1f.2) Intel Corporation Wellsburg AHCI Controller
(0000:01:00.0) Broadcom MegaRAID SAS Fury Controller
Second Level Lun ID  (0000:81:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter
Second Level Lun ID  (0000:81:00.1) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter
(0000:81:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter
----

==	Validate NVMe/FC

.Steps

. Verify that NVMe/FC adapter is listed on the ESXi host:
+
----
# esxcli nvme adapter list
Adapter  Adapter Qualified Name           Transport Type  Driver      Associated Devices
-------  -------------------------------  --------------  ----------  ------------------
vmhba64  aqn:qlnativefc:21000024ff7f4a50  FC              qlnativefc
vmhba65  aqn:qlnativefc:21000024ff7f4a51  FC              qlnativefc
----

. Verify that the NVMe/FC namespaces are correctly created:
+
The UUIDs in the following example represent the NVMe/FC namespace devices.
+
----
# esxcfg-mpath -b
uuid.116cb7ed9e574a0faf35ac2ec115969d : NVMe Fibre Channel Disk (uuid.116cb7ed9e574a0faf35ac2ec115969d)
   vmhba64:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:05:d0:39:ea:3a:b2:1f
   vmhba64:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:07:d0:39:ea:3a:b2:1f
   vmhba65:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:08:d0:39:ea:3a:b2:1f
   vmhba65:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:06:d0:39:ea:3a:b2:1f
----
+
[NOTE] 
====
In ONTAP 9.7, the default block size for a NVMe/FC namespace is 4K. This default size is not compatible with ESXi. Therefore, when creating namespaces for ESXi, you must set the namespace block size as 512b. You can do this using the `vserver nvme namespace create` command.

Example,

`vserver nvme namespace create -vserver vs_1 -path /vol/nsvol/namespace1 -size 100g -ostype vmware -block-size 512B`

Refer to the link:https://docs.netapp.com/ontap-9/index.jsp?topic=%2Fcom.netapp.doc.dot-cm-cmpr%2FGUID-5CB10C70-AC11-41C0-8C16-B4D0DF916E9B.html[ONTAP 9 Command man pages^] for additional details.
====

. Verify the status of the individual ANA paths of the respective NVMe/FC namespace devices:
+
----
esxcli storage hpp path list -d uuid.4c4a60628ad44587bee44f6ccedcd3b2

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2005d039ea3ab21f-uuid.4c4a60628ad44587bee44f6ccedcd3b2
   Runtime Name: vmhba64:C0:T0:L1
   Device: uuid.4c4a60628ad44587bee44f6ccedcd3b2
   Device Display Name: NVMe Fibre Channel Disk (uuid.4c4a60628ad44587bee44f6ccedcd3b2)
   Path State: active unoptimized
   Path Config: {TPG_id=1280,TPG_state=ANO,RTP_id=1280,health=UP}
 
fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2008d039ea3ab21f-uuid.4c4a60628ad44587bee44f6ccedcd3b2
   Runtime Name: vmhba65:C0:T1:L1
   Device: uuid.4c4a60628ad44587bee44f6ccedcd3b2
   Device Display Name: NVMe Fibre Channel Disk (uuid.4c4a60628ad44587bee44f6ccedcd3b2)
   Path State: active
   Path Config: {TPG_id=1537,TPG_state=AO,RTP_id=1537,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2006d039ea3ab21f-uuid.4c4a60628ad44587bee44f6ccedcd3b2
   Runtime Name: vmhba65:C0:T0:L1
   Device: uuid.4c4a60628ad44587bee44f6ccedcd3b2
   Device Display Name: NVMe Fibre Channel Disk (uuid.4c4a60628ad44587bee44f6ccedcd3b2)
   Path State: active unoptimized
   Path Config: {TPG_id=1536,TPG_state=ANO,RTP_id=1536,health=UP}


fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2007d039ea3ab21f-uuid.4c4a60628ad44587bee44f6ccedcd3b2
   Runtime Name: vmhba64:C0:T1:L1
   Device: uuid.4c4a60628ad44587bee44f6ccedcd3b2
   Device Display Name: NVMe Fibre Channel Disk (uuid.4c4a60628ad44587bee44f6ccedcd3b2)
   Path State: active
   Path Config: {TPG_id=1281,TPG_state=AO,RTP_id=1281,health=UP}
----

== Configure NVMe/TCP

In ESXi 8.0, the required NVMe/TCP modules will be loaded by default. For configuring the network and the NVMe/TCP adapter, refer to the VMware vSphere documentation.

== Validate NVMe/TCP

.Steps

. Verify the status of the NVMe/TCP adapter.
+
----
esxcli nvme adapter list
Adapter  Adapter Qualified Name           Transport Type  Driver   Associated Devices
-------  -------------------------------  --------------  -------  ------------------
vmhba65  aqn:nvmetcp:ec-2a-72-0f-e2-30-T  TCP             nvmetcp  vmnic0
vmhba66  aqn:nvmetcp:34-80-0d-30-d1-a0-T  TCP             nvmetcp  vmnic2
vmhba67  aqn:nvmetcp:34-80-0d-30-d1-a1-T  TCP             nvmetcp  vmnic3
----

. To list the NVMe/TCP connections, use the following command:
+
----
esxcli nvme controller list
Name    Controller Number  Adapter  Transport Type  Is Online  Is VVOL
--------------------------------------------------------------------------------------------------------  -----------------  -------  -----
nqn.2014-08.org.nvmexpress.discovery#vmhba66#192.168.100.123:8009    259  vmhba66  TCP                  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba67#192.168.101.125:8009                                                       262  vmhba67  TCP                  true    false
                     nqn.2014-08.org.nvmexpress.discovery#vmhba67#192.168.101.124:8009                                                       263  vmhba67  TCP                  true    false
                     nqn.2014-08.org.nvmexpress.discovery#vmhba66#192.168.100.124:8009                                                       264  vmhba66  TCP                  true    false
                     nqn.1992-08.com.netapp:sn.5358d36afba111ec93fcd039ea345406:subsystem.tcp_ss#vmhba66#192.168.100.123:4420                266  vmhba66  TCP                  true    false
                     nqn.1992-08.com.netapp:sn.5358d36afba111ec93fcd039ea345406:subsystem.tcp_ss#vmhba67#192.168.101.125:4420                267  vmhba67  TCP                  true    false
                     nqn.1992-08.com.netapp:sn.5358d36afba111ec93fcd039ea345406:subsystem.tcp_ss#vmhba66#192.168.100.124:4420                270  vmhba66  TCP                  true    false
                     nqn.1992-08.com.netapp:sn.5358d36afba111ec93fcd039ea345406:subsystem.tcp_ss#vmhba67#192.168.101.124:4420                271  vmhba67  TCP                  true    false

----

. To list the number of paths to an NVMe namespace, use the following command:
+
----
esxcli storage hpp path list -d uuid.7caba9dea0b34b27b1c14722dc36b0ad
tcp.vmnic3:34:80:0d:30:d1:a1-tcp.192.168.101.125:4420-uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Runtime Name: vmhba67:C0:T0:L1
   Device: uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Device Display Name: NVMe TCP Disk (uuid.7caba9dea0b34b27b1c14722dc36b0ad)
   Path State: active
   Path Config: {TPG_id=1536,TPG_state=AO,RTP_id=1536,health=UP}

tcp.vmnic2:34:80:0d:30:d1:a0-tcp.192.168.100.124:4420-uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Runtime Name: vmhba66:C0:T1:L1
   Device: uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Device Display Name: NVMe TCP Disk (uuid.7caba9dea0b34b27b1c14722dc36b0ad)
   Path State: active unoptimized
   Path Config: {TPG_id=1281,TPG_state=ANO,RTP_id=1281,health=UP}

tcp.vmnic2:34:80:0d:30:d1:a0-tcp.192.168.100.123:4420-uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Runtime Name: vmhba66:C0:T0:L1
   Device: uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Device Display Name: NVMe TCP Disk (uuid.7caba9dea0b34b27b1c14722dc36b0ad)
   Path State: active
   Path Config: {TPG_id=1280,TPG_state=AO,RTP_id=1280,health=UP}

tcp.vmnic3:34:80:0d:30:d1:a1-tcp.192.168.101.124:4420-uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Runtime Name: vmhba67:C0:T1:L1
   Device: uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Device Display Name: NVMe TCP Disk (uuid.7caba9dea0b34b27b1c14722dc36b0ad)
   Path State: active unoptimized
   Path Config: {TPG_id=1537,TPG_state=ANO,RTP_id=1537,health=UP}
----

== Known issue

*	ESXi 8.0 (and later) NVMe/FC support is available starting with ONTAP 9.9.1 P3 and later because key NVMe abort fixes (issued by ESXi 8.0 and later) are available starting with ONTAP 9.9.1 P3. Refer to the respective burt public report at https://mysupport.netapp.com/site/bugs-online/product/ONTAP/BURT/1420654 for details.


== Related Links

link:https://docs.netapp.com/us-en/netapp-solutions/virtualization/vsphere_ontap_ontap_for_vsphere.html[TR-4597-VMware vSphere with ONTAP^]
link:https://kb.vmware.com/s/article/2031038[VMware vSphere 5.x, 6.x and 7.x support with NetApp MetroCluster  (2031038)^]
link:https://kb.vmware.com/s/article/83370[VMware vSphere 6.x and 7.x support with NetApp® SnapMirror® Business Continuity (SM-BC)^]

//BURT 1525630 20-Jan-2023