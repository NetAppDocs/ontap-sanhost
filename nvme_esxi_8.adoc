---
sidebar: sidebar
permalink: nvme_esxi_8.html
keywords: nvme, esxi, ontap, nvme/fc, hypervisor
summary: Describes how to configure NVMe-oF for ESXi 7.x with ONTAP
---

= NVMe-oF Host Configuration for ESXi 8.x with ONTAP
:toc: macro
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

== Supportability

* Beginning with ONTAP 9.7, NVMe over Fibre Channel (NVMe/FC) support is added for VMware vSphere releases.
* Beginning with ONTAP 9.10.1, NVMe/TCP feature is supported for ONTAP.


== Features

*	ESXi initiator host can run both NVMe/FC and FCP traffic through the same adapter ports. See the link:https://hwu.netapp.com/Home/Index[Hardware Universe^] for a list of supported FC adapters and controllers. See the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix Tool^] for the most current list of supported configurations and versions.

*	Beginning with ONTAP 9.9.1 P3, NVMe/FC feature is supported for ESXi 8 and later.

*	For ESXi 8.0 and later releases, HPP (high performance plugin) is the default plugin for NVMe devices.

==	Known limitations

The following configurations are not supported:

* RDM mapping
* VVols


== Enable NVMe/FC

You need to check the ESXi host NQN string and verify that it matches with the host NQN string for the corresponding subsystem on the ONTAP array. Use the following command:

----
# esxcli nvme  info get
----
Example output:
----
Host NQN: nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----

----
# vserver nvme subsystem host show -vserver nvme_fc
----
Example output:
----
Vserver Subsystem Host NQN
------- --------- ----------------------------------------------------------
nvme_fc nvme_ss  nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----

== Configure Broadcom/Emulex

.Steps

. Check whether the configuration is supported with required driver or firmware by referring to link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix Tool^].

. The lpfc driver in vSphere 8.x has the NVMe/FC capability enabled by default.

//Set the `lpfc` driver parameter `lpfc_enable_fc4_type=3` for enabling NVMe/FC support in the `lpfc` driver and reboot the host.

//[NOTE] 
//====
//* With vSphere 8.0, the `brcmnvmefc` driver is no longer available. The `lpfc` driver now includes the NVMe over Fibre Channel (NVMe/FC) //functionality previously delivered with the `brcmnvmefc` driver.

//* The `lpfc_enable_fc4_type=3` parameter is set by default for the LPe35000-series adapters. You need to execute the following command to set the //parameter manually for LPe32000-series and LPe31000-series adapters.
+
//----
//# esxcli system module parameters set -m lpfc -p lpfc_enable_fc4_type=3
//# esxcli system module parameters list  -m lpfc | grep lpfc_enable_fc4_type

//lpfc_enable_fc4_type              int            Defines what FC4 types are supported
//----
//+
//----
//# esxcli storage core adapter list
//----
//+
//Example output
//+
//----

//HBA Name  Driver      Link State  UID                                   
//--------  ----------  ----------  ------------------------------------  

//vmhba0    vmw_ahci    link-n/a    sata.vmhba0       
//vmhba1    vmw_ahci    link-n/a    sata.vmhba1        
//vmhba2    lsi_mr3     link-n/a    sas.5003005702854d00       
//vmhba3    qlnativefc  link-up     fc.20000024ff7f4a50:21000024ff7f4a50  
//vmhba4    qlnativefc  link-up     fc.20000024ff7f4a51:21000024ff7f4a51  
//vmhba64   qlnativefc  link-up     fc.20000024ff7f4a50:21000024ff7f4a50                       
//vmhba65   qlnativefc  link-up     fc.20000024ff7f4a51:21000024ff7f4a51 

//Capabilities         Description
//-------------------------------------
//(0000:00:11.4) Intel Corporation Wellsburg AHCI Controller
//(0000:00:1f.2) Intel Corporation Wellsburg AHCI Controller
//(0000:01:00.0) Broadcom MegaRAID SAS Fury Controller
//Second Level Lun ID  (0000:81:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter
//econd Level Lun ID  (0000:81:00.1) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter
//(0000:81:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter
//(0000:81:00.1) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter

//----
====

== Configure Marvell/QLogic

.Steps

. Check whether the configuration is supported with required driver or firmware by referring to link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix Tool^].

. Set the `qlnativefc` driver parameter `ql2xnvmesupport=1` to enable NVMe/FC support in the `qlnativefc` driver and reboot the host.
+
`# esxcfg-module -s 'ql2xnvmesupport=1' qlnativefc`
+
The `qlnativefc` driver parameter is set by default for the Qle 277x-series adapters. You must execute the following command to set the parameter manually for Qle 277x series adapters.
+
----
esxcfg-module -l | grep qlnativefc
qlnativefc               4    1820
----

. The qlnativefc driver in vSphere 8.x has the NVMe/FC capability enabled by default.


==	Validate NVMe/FC

.Steps

. Verify that the NVMe/FC adapter is listed on the ESXi host:
+
----
# esxcli nvme adapter list
----
+
Example output:
+
----

Adapter  Adapter Qualified Name           Transport Type  Driver      Associated Devices
-------  -------------------------------  --------------  ----------  ------------------
vmhba64  aqn:lpfc:100000109b579f11        FC              lpfc
vmhba65  aqn:lpfc:100000109b579f12        FC              lpfc
vmhba66  aqn:qlnativefc:2100f4e9d456e286  FC              qlnativefc
vmhba67  aqn:qlnativefc:2100f4e9d456e287  FC              qlnativefc
----

. Verify that the NVMe/FC namespaces are correctly created:
+
The UUIDs in the following example represent the NVMe/FC namespace devices.
+
[subs=+quotes]
----
# esxcfg-mpath -b
uuid.116cb7ed9e574a0faf35ac2ec115969d : NVMe Fibre Channel Disk (*uuid.116cb7ed9e574a0faf35ac2ec115969d*)
   vmhba64:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:05:d0:39:ea:3a:b2:1f
   vmhba64:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:07:d0:39:ea:3a:b2:1f
   vmhba65:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:08:d0:39:ea:3a:b2:1f
   vmhba65:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:06:d0:39:ea:3a:b2:1f
----
+
[NOTE] 
====
In ONTAP 9.7, the default block size for an NVMe/FC namespace is 4K. This default size is not compatible with ESXi. Therefore, when creating namespaces for ESXi, you need to set the namespace block size as *512B*. You can do this using the `vserver nvme namespace create` command.

Example,

`vserver nvme namespace create -vserver vs_1 -path /vol/nsvol/namespace1 -size 100g -ostype vmware -block-size 512B`

Refer to the link:https://docs.netapp.com/ontap-9/index.jsp?topic=%2Fcom.netapp.doc.dot-cm-cmpr%2FGUID-5CB10C70-AC11-41C0-8C16-B4D0DF916E9B.html[ONTAP 9 Command man pages^] for additional details.
====

. Verify the status of the individual ANA paths of the respective NVMe/FC namespace devices:
+
[subs=+quotes]
----
# esxcli storage hpp path list -d uuid.df960bebb5a74a3eaaa1ae55e6b3411d

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2005d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2008d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2006d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2007d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

----

== Configure NVMe/TCP

In ESXi 8.0, the required NVMe/TCP modules is loaded by default. To configure the network and the NVMe/TCP adapter, refer to the VMware vSphere documentation.

== Validate NVMe/TCP

.Steps

. Verify the status of the NVMe/TCP adapter:
+
----
esxcli nvme adapter list
----
+
Example output:
+
----
Adapter  Adapter Qualified Name           Transport Type  Driver   Associated Devices
-------  -------------------------------  --------------  -------  ------------------
vmhba65  aqn:nvmetcp:ec-2a-72-0f-e2-30-T  TCP             nvmetcp  vmnic0
vmhba66  aqn:nvmetcp:34-80-0d-30-d1-a0-T  TCP             nvmetcp  vmnic2
vmhba67  aqn:nvmetcp:34-80-0d-30-d1-a1-T  TCP             nvmetcp  vmnic3
----

. Retrieve a list of NVMe/TCP connections:
+
----
esxcli nvme controller list
----
+
Example output:
+
----
Name                                                  Controller Number  Adapter  Transport Type  Is Online  Is VVOL
---------------------------------------------------------------------------------------------------------  -----------------  -------  
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.166:8009  256  vmhba64  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.165:4420 258  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.168:4420 259  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.166:4420 260  vmhba64  TCP  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.165:8009  261  vmhba64  TCP                  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba65#192.168.100.155:8009  262  vmhba65  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.167:4420 264  vmhba64  TCP  true    false

----

. Retrieve a list of the number of paths to an NVMe namespace:
+
[subs=+quotes]
----
esxcli storage hpp path list -d *uuid.7caba9dea0b34b27b1c14722dc36b0ad*
----
+
Example output:
+
----
tcp.vmnic3:34:80:0d:30:d1:a1-tcp.192.168.101.125:4420-uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Runtime Name: vmhba67:C0:T0:L1
   Device: uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Device Display Name: NVMe TCP Disk (uuid.7caba9dea0b34b27b1c14722dc36b0ad)
   Path State: active
   Path Config: {TPG_id=1536,TPG_state=AO,RTP_id=1536,health=UP}

tcp.vmnic2:34:80:0d:30:d1:a0-tcp.192.168.100.124:4420-uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Runtime Name: vmhba66:C0:T1:L1
   Device: uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Device Display Name: NVMe TCP Disk (uuid.7caba9dea0b34b27b1c14722dc36b0ad)
   Path State: active unoptimized
   Path Config: {TPG_id=1281,TPG_state=ANO,RTP_id=1281,health=UP}

tcp.vmnic2:34:80:0d:30:d1:a0-tcp.192.168.100.123:4420-uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Runtime Name: vmhba66:C0:T0:L1
   Device: uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Device Display Name: NVMe TCP Disk (uuid.7caba9dea0b34b27b1c14722dc36b0ad)
   Path State: active
   Path Config: {TPG_id=1280,TPG_state=AO,RTP_id=1280,health=UP}

tcp.vmnic3:34:80:0d:30:d1:a1-tcp.192.168.101.124:4420-uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Runtime Name: vmhba67:C0:T1:L1
   Device: uuid.7caba9dea0b34b27b1c14722dc36b0ad
   Device Display Name: NVMe TCP Disk (uuid.7caba9dea0b34b27b1c14722dc36b0ad)
   Path State: active unoptimized
   Path Config: {TPG_id=1537,TPG_state=ANO,RTP_id=1537,health=UP}
----

== Known issue

*	ESXi 8.0 (and later) NVMe/FC support is available starting with ONTAP 9.9.1 P3 and later because key NVMe abort fixes (issued by ESXi 8.0 and later) are available starting with ONTAP 9.9.1 P3. Refer to the respective burt public report at https://mysupport.netapp.com/site/bugs-online/product/ONTAP/BURT/1420654 for details.


== Related Links

link:https://docs.netapp.com/us-en/netapp-solutions/virtualization/vsphere_ontap_ontap_for_vsphere.html[TR-4597-VMware vSphere with ONTAP^]
link:https://kb.vmware.com/s/article/2031038[VMware vSphere 5.x, 6.x and 7.x support with NetApp MetroCluster  (2031038)^]
link:https://kb.vmware.com/s/article/83370[VMware vSphere 6.x and 7.x support with NetApp® SnapMirror® Business Continuity (SM-BC)^]

//BURT 1525630 20-Jan-2023