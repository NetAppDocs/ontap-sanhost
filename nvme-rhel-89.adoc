---
sidebar: sidebar
permalink: nvme-rhel-89.html
keywords: nvme, linux, rhel, red hat, enterprise
summary: How to Configure NVMe-oF Host for RHEL 8.9 with ONTAP
---
= Configure RHEL 8.9 for NVMe-oF with ONTAP storage
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
include::_include/nvme/nvme-introduction.adoc[]

This document describes how to configure NVMe over Fabrics (NVMe-oF) hosts for RHEL 8.9. For more support and feature information, see link:hu-nvme-index.html[NVME-oF Overview^]. NVMe-oF with RHEL 8.9 has the following known limitations:

* SAN booting using the NVMe-oF protocol is not currently supported.
* In-kernel NVMe multipath is disabled by default on NVMe-oF hosts in RHEL 8.9; you must enable it manually.
* NVMe/TCP is available as a technology preview due to known issues.

== Step 1: Optionally, enable SAN booting

include::_include/nvme/enable-san-booting.adoc[]

== Step 2: Verify the software version and NVMe configuration
 
include::_include/nvme/verify-software-version-introduction.adoc[]

.Steps

. Install RHEL 8.9 on the server. After the installation is complete, verify that you are running the required RHEL 8.9 kernel: 
+
[source,cli]
----
uname -r
----
+
Example RHEL kernel version:
+
----
4.18.0-513.5.1.el8_9.x86_64
----

. Install the `nvme-cli` package:
+
[source,cli]
----
rpm -qa|grep nvme-cli
----
+
The following example shows an nvme-cli package version:
+
----
nvme-cli-1.16-9.el8.x86_64
----

. Install the `libnvme` package:
+
[source,cli]
----
rpm -qa|grep libnvme
----
+
The following example shows an libnvme package version:
+
----
libnvme-1.4-3.el8.x86_64
----

. Enable in-kernel NVMe multipath:
+
[source,cli]
----
grubby --args=nvme_core.multipath=Y --update-kernel /boot/vmlinuz-4.18.0-513.5.1.el8_9.x86_64
----

. On the RHEL 8.9 host, check the `hostnqn` string at `/etc/nvme/hostnqn`:
+
[source,cli]
----
cat /etc/nvme/hostnqn
----
+
The following example shows an `hostnqn` version:
+
----
nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0032-3410-8035-b8c04f4c5132
----

. Verify that the `hostnqn` string matches the `hostnqn` string for the corresponding subsystem on the ONTAP storage system:
+
[source,cli]
----
::> vserver nvme subsystem host show -vserver vs_fcnvme_141
----
+
.Show example
[%collapsible]
====
----
Vserver     Subsystem          Host NQN
----------- --------------- ----------------------------------------------------------
vs_nvme101   rhel_101_QLe2772    nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0032-3410-8035-b8c04f4c5132
----
====
+
[NOTE]
If the `hostnqn` strings do not match, use the `vserver modify` command to update the `hostnqn` string on your corresponding ONTAP storage system subsystem to match the `hostnqn` string from `/etc/nvme/hostnqn` on the host.

. Reboot the host.
+
[NOTE]
====
To run both NVMe and SCSI traffic on the same host, NetApp recommends using the in-kernel NVMe multipath for ONTAP namespaces and dm-multipath for ONTAP LUNs. To prevent dm-multipath from claiming ONTAP namespace devices, exclude them by adding the `enable_foreign` setting to the `/etc/multipath.conf` file:	

[source,cli]
----
cat /etc/multipath.conf
defaults {
        enable_foreign     NONE
}
----
====

== Step 3: Configure NVMe/FC and NVMe/TCP

Configure NVMe/FC with Broadcom/Emulex or Marvell/QLogic adapters, or configure NVMe/TCP using manual discovery and connect operations. 

[role="tabbed-block"]
====
.FC - Broadcom/Emulex
--

Configure NVMe/FC for a Broadcom/Emulex adapter.

.Steps

. Verify that you are using the supported adapter model:

.. Display the model names:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/modelname
----
+
You should see output similar to the following example:
+
----
LPe32002-M2
LPe32002-M2
----

.. Display the model descriptions:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/modeldesc
----
+
You should see output similar to the following example:
+
----
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
----

. Verify that you are using the recommended Broadcom `lpfc` firmware and inbox driver:

.. Display the firmware version:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/fwrev
----
+
The command returns the firmware versions:
+
----
14.2.539.16, sli-4:2:c
14.2.539.16, sli-4:2:c
----

.. Display the inbox driver version:
+
[source,cli]
----
cat /sys/module/lpfc/version
----
+
The following example shows a driver version:
+
----
0:14.0.0.21
----

+
For the current list of supported adapter driver and firmware versions, see the link:https://mysupport.netapp.com/matrix/[Interoperability Matrix Tool^].

. Verify that `lpfc_enable_fc4_type` is set to `3`:
+
[source,cli]
----
cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
----

. Verify that you can view your initiator ports:
+
[source,cli]
----
cat /sys/class/fc_host/host*/port_name
----
+
You should see output similar to the following example:
+
----
0x10000090fae0ec88
0x10000090fae0ec89
----

. Verify that your initiator ports are online:
+
[source,cli]
----
cat /sys/class/fc_host/host*/port_state
----
+
You should see the following output:
+
----
Online
Online
----

. Verify that the NVMe/FC initiator ports are enabled and that the target ports are visible:
+
[source,cli]
----
cat /sys/class/scsi_host/host*/nvme_info
----
+
.Show example
[%collapsible]
=====
[subs=+quotes]
----
NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x10000090fae0ec88 WWNN x20000090fae0ec88 DID x0a1300 *ONLINE*
NVME RPORT       WWPN x2049d039ea36a105 WWNN x2048d039ea36a105 DID x0a0c0a *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 0000000024 Cmpl 0000000024 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 00000000000001aa Issue 00000000000001ab OutIO 0000000000000001
        abort 00000002 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00000002 Err 00000003

NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x10000090fae0ec89 WWNN x20000090fae0ec89 DID x0a1200 *ONLINE*
NVME RPORT       WWPN x204ad039ea36a105 WWNN x2048d039ea36a105 DID x0a080a *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 0000000024 Cmpl 0000000024 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 00000000000001ac Issue 00000000000001ad OutIO 0000000000000001
        abort 00000002 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00000002 Err 00000003
----
=====

--
.FC - Marvell/QLogic
--

Configure NVMe/FC for a Marvell/QLogic adapter

.Steps

. Verify that you are using the supported adapter driver and firmware versions:
+
[source,cli]
----
cat /sys/class/fc_host/host*/symbolic_name
----
+
The following example shows driver and firmware versions:
+
----
QLE2742 FW: v9.10.11 DVR: v10.02.08.200-k
QLE2742 FW: v9.10.11 DVR: v10.02.08.200-k
----

. Verify that `ql2xnvmeenable` is set. This enables the Marvell adapter to function as an NVMe/FC initiator:
+
[source,cli]
----
cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
----
+
The expected output is 1.

--
.TCP
--

The NVMe/TCP protocol doesn't support the auto-connect operation. Instead, you can discover the NVMe/TCP subsystems and namespaces by performing the NVMe/TCP `connect` or `connect-all` operations manually.

.Steps

. Check that the initiator port can get the discovery log page data across the supported NVMe/TCP LIFs:
+
[source,cli]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
.Show example
[%collapsible]
=====
[subs=+quotes]
----
nvme discover -t tcp -w 192.168.111.79 -a 192.168.111.14 -l 1800

Discovery Log Number of Records 8, Generation counter 18
=====Discovery Log Entry 0======
trtype:  tcp
adrfam:  ipv4
subtype: unrecognized
treq:    not specified.
portid:  0
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.154a5833c78c11ecb069d039ea359e4b: discovery
traddr:  192.168.211.15
sectype: none
=====Discovery Log Entry 1======
trtype:  tcp
adrfam:  ipv4
subtype: unrecognized
treq:    not specified.
portid:  1
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.154a5833c78c11ecb069d039ea359e4b: discovery
traddr:  192.168.111.15
sectype: none
----
=====

. Verify that the other NVMe/TCP initiator-target LIF combinations can successfully retrieve discovery log page data: 
+
[source,cli]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
.Show example
[%collapsible]
=====
[subs=+quotes]
----
nvme discover -t tcp -w 192.168.111.79 -a 192.168.111.14
nvme discover -t tcp -w 192.168.111.79 -a 192.168.111.15
nvme discover -t tcp -w 192.168.211.79 -a 192.168.211.14
nvme discover -t tcp -w 192.168.211.79 -a 192.168.211.15
----
=====

. Run the `nvme connect-all` command across all the supported NVMe/TCP initiator-target LIFs across the nodes:
+
[source,cli]
----
nvme connect-all -t tcp -w host-traddr -a traddr -1 1800
----
+
.Show example
[%collapsible]
=====
[subs=+quotes]
----
nvme connect-all -t tcp -w 192.168.111.79 -a 192.168.111.14 -l 1800
nvme connect-all -t tcp -w 192.168.111.79 -a 192.168.111.15 -l 1800
nvme connect-all -t tcp -w 192.168.211.79 -a 192.168.211.14 -l 1800
nvme connect-all -t tcp -w 192.168.211.79 -a 192.168.211.15 -l 1800
----			
=====

--
====

== Step 4: Optionally, enable 1MB I/O for NVMe/FC

include::_include/nvme/nvme-enabling-broadcom-1mb-size.adoc[] 

== Step 5: Validate NVMe-oF

include::_include/nvme/nvme-validate-nvme-of.adoc[]

. Verify that the controller state of each path is live and has the correct ANA status:
+
[role="tabbed-block"]
=====
.NVMe/FC
--
[source,cli]
----
nvme list-subsys /dev/nvme3n1 
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.8e501f8ebafa11ec9b99d039ea359e4b:subsystem.rhel_163_Qle2742 
+- nvme0 *fc* traddr=nn-0x204dd039ea36a105:pn-0x2050d039ea36a105 host_traddr=nn-0x20000024ff7f4994:pn-0x21000024ff7f4994 *live non-optimized*
+- nvme1 *fc* traddr=nn-0x204dd039ea36a105:pn-0x2050d039ea36a105 host_traddr=nn-0x20000024ff7f4994:pn-0x21000024ff7f4994 *live non-optimized*
+- nvme2 *fc* traddr=nn-0x204dd039ea36a105:pn-0x204fd039ea36a105 host_traddr=nn-0x20000024ff7f4995:pn-0x21000024ff7f4995 *live optimized*
+- nvme3 *fc* traddr=nn-0x204dd039ea36a105:pn-0x204ed039ea36a105 host_traddr=nn-0x20000024ff7f4994:pn-0x21000024ff7f4994 *live optimized* 
----
====
--
.NVMe/TCP
--
[source,cli]
----
nvme list-subsys /dev/nvme0n1 
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.154a5833c78c11ecb069d039ea359e4b:subsystem.rhel_tcp_165\
+- nvme0 *tcp* traddr=192.168.111.15 trsvcid=4420 host_traddr=192.168.111.79 *live non-optimized*
+- nvme1 *tcp* traddr=192.168.111.14 trsvcid=4420 host_traddr=192.168.111.79 *live optimized*
+- nvme2 *tcp* traddr=192.168.211.15 trsvcid=4420 host_traddr=192.168.211.79 *live non-optimized*
+- nvme3 *tcp* traddr=192.168.211.14 trsvcid=4420 host_traddr=192.168.211.79 *live optimized*
----
====
--
=====

. Verify that the NetApp plug-in displays the correct values for each ONTAP namespace device:
+
[role="tabbed-block"]
=====
.Column
--
[source,cli]
----
nvme netapp ontapdevices -o column
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
Device        Vserver   Namespace Path                                                          
----------------------- ------------------------------ 
/dev/nvme0n1 vs_tcp79           /vol/vol1/ns

              
NSID       UUID                                   Size
------------------------------------------------------------
1          aa197984-3f62-4a80-97de-e89436360cec	21.47GB
----
====
--
.JSON
--
[source,cli]
----
nvme netapp ontapdevices -o json
----

.Show example
[%collapsible]
====
[subs=+quotes]
----
{
  "ONTAPdevices”: [
    {
      "Device”: "/dev/nvme0n1",
      "Vserver”: "vs_tcp79",
      "Namespace Path”: "/vol/vol1/ns",
      "NSID”: 1,
      "UUID”: "aa197984-3f62-4a80-97de-e89436360cec",
      "Size”: "21.47GB",
      "LBA_Data_Size”: 4096,
      "Namespace Size" : 5242880
    },
]

}
----
====
--
=====

== Step 6: Review the known issues

These are the known issues:

[cols="20,40,40",options="header"]
|===
|NetApp Bug ID	|Title	|Description	

|link:https://mysupport.netapp.com/site/bugs-online/product/HOSTUTILITIES/BURT/1479047[1479047^]	|RHEL 8.9 NVMe-oF hosts create duplicate persistent discovery controllers (PDCs)	|On NVMe-oF hosts, you can use the "nvme discover -p" command to create PDCs. When this command is used, only one PDC should be created per initiator-target combination.  However, if you are running RHEL 8.9 on an NVMe-oF host, a duplicate PDC is created each time "nvme discover -p" is executed. This leads to unnecessary usage of resources on both the host and the target.
|===

// 2024 SEP 2, ONTAPDOC-2345