---
sidebar: sidebar
permalink: nvme_sles15_sp4.html
keywords: nvme, linux, suse, sles, 15, sp4, server, enterprise
summary: Describes how to configure NVMe/FC for SUSE Linux Enterprise Server 15 SP4 with ONTAP
---

= NVMe-oF Host Configuration for SUSE Linux Enterprise Server 15 SP4 with ONTAP
:toc: macro
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/
:source-highlighter: highlighter.js

   
== Supportability

NVMe over Fabrics or NVMe-oF (including NVMe/FC and other transports) is supported with SUSE Linux Enterprise Server 15 SP4 (SLES15 SP4) with ANA (Asymmetric Namespace Access). ANA is the ALUA equivalent in NVMe-oF environment, and is currently implemented with in-kernel NVMe Multipath. The details for enabling NVMe-oF with in-kernel NVMe Multipath using ANA on SLES15 SP4 and ONTAP as the target has been documented here.

include::_include/hu/reuse_hu_cloud_note.adoc[]

== Features

* SLES15 SP4 supports NVMe/FC and other transports.

* There is no `sanlun` support for NVMe-oF. Therefore, there is no LUHU support for NVMe-oF on SLES15 SP4. You can rely on the NetApp plug-in included in the native `nvme-cli` for the same instead. This should work for all NVMe-oF transports.

* Both NVMe and SCSI traffic can be run on the same co-existent host. In fact, that is expected to be the commonly deployed host configuration. Therefore, for SCSI, you might configure `dm-multipath` as usual for SCSI LUNs resulting in mpath devices, whereas NVMe multipath might be used to configure NVMe-oF multipath devices on the host.

* NVMe in-band authentication is supported starting SLES15 SP4. 

== Known limitations

There are no known limitations.

== Configuration Requirements

Refer to the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix^]  for accurate details regarding supported configurations.

== Enable in-kernel NVMe Multipath

In-kernel NVMe multipath is already enabled by default on SLES hosts such as SLES15 SP4. Therefore, no additional setting is required here. Refer to the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix^]  for accurate details regarding supported configurations.

== NVMe-oF initiator packages

Refer to the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix^]  for accurate details regarding supported configurations.

* Verify that you have the requisite kernel and `nvme-cli` maintenance update (MU) packages installed on the SLES15 SP4 MU host.
+
Example:
+
----
# uname -r
5.14.21-150400.24.11-default
----
+
----
# rpm -qa|grep nvme-cli
nvme-cli-2.0-150400.1.6.x86_64
----
+
The above nvme-cli MU package now includes the following:

** *NVMe/FC auto-connect scripts* - Required for NVMe/FC auto-(re)connect when underlying paths to the namespaces are restored as well as during the host reboot:
+
----
# rpm -ql nvme-cli-1.13-3.3.1.x86_64
     /etc/nvme
     /etc/nvme/discovery.conf
     /etc/nvme/hostid
     /etc/nvme/hostnqn
     /usr/lib/systemd/system/nvmefc-boot-connections.service
     /usr/lib/systemd/system/nvmf-autoconnect.service
     /usr/lib/systemd/system/nvmf-connect.target
...
----

** *ONTAP udev rule* - New udev rule to ensure NVMe multipath round-robin loadbalancer default applies to all ONTAP namespaces:
+
----
# rpm -ql nvme-cli-1.13-3.3.1.x86_64
/etc/nvme
/etc/nvme/discovery.conf
/etc/nvme/hostid
/etc/nvme/hostnqn
/usr/lib/systemd/system/nvmefc-boot-connections.service
/usr/lib/systemd/system/nvmf-autoconnect.service
/usr/lib/systemd/system/nvmf-connect.target
/usr/lib/systemd/system/nvmf-connect@.service
/usr/lib/udev/rules.d/70-nvmf-autoconnect.rules
/usr/lib/udev/rules.d/71-nvmf-iopolicy-netapp.rules
...
# cat /usr/lib/udev/rules.d/71-nvmf-iopolicy-netapp.rules
# Enable round-robin for NetApp ONTAP and NetApp E-Series
ACTION=="add", SUBSYSTEM=="nvme-subsystem", ATTR{model}=="NetApp ONTAP Controller", ATTR{iopolicy}="round-robin"
ACTION=="add", SUBSYSTEM=="nvme-subsystem", ATTR{model}=="NetApp E-Series", ATTR{iopolicy}="round-robin"
----

** *NetApp plug-in for ONTAP devices* - The existing NetApp plug-in has now been modified to handle ONTAP namespaces as well.

* Check the `hostnqn` string at  `/etc/nvme/hostnqn` on the host and ensure that it properly matches with the `hostnqn` string for the corresponding subsystem on the ONTAP array. For example,
+
----
# cat /etc/nvme/hostnqn
nqn.2014-08.org.nvmexpress:uuid:60c23e12-15f4-11e5-a5ca-98be942448b2
::> vserver nvme subsystem host show -vserver vs_fcnvme_145
Vserver     Subsystem      Host NQN
-------     ---------      ----------------------------------
vs_nvme_145 nvme_145_1 nqn.2014-08.org.nvmexpress:uuid:c7b07b16-a22e-41a6-a1fd-cf8262c8713f
            nvme_145_2 nqn.2014-08.org.nvmexpress:uuid:c7b07b16-a22e-41a6-a1fd-cf8262c8713f
            nvme_145_3 nqn.2014-08.org.nvmexpress:uuid:c7b07b16-a22e-41a6-a1fd-cf8262c8713f
            nvme_145_4 nqn.2014-08.org.nvmexpress:uuid:c7b07b16-a22e-41a6-a1fd-cf8262c8713f
            nvme_145_5 nqn.2014-08.org.nvmexpress:uuid:c7b07b16-a22e-41a6-a1fd-cf8262c8713f
5 entries were displayed.
----

Proceed with the following steps depending on the FC adapter being used on the host.

== Update NVMe-oF initiator packages

.Steps

. Install the SLES15 SP4 MU `kernel-default-5.14.21-150400.24.21.2` package to avail the latest kernel fixes.
+
`# rpm -ivh kernel-default-5.14.21-150400.24.21.2.x86_64.rpm`
+
[NOTE]
====
If the above `rpm -ivh` command fails with a newer package already installed message, then proceed to install the following using the --oldpackage option, for example:
+
`# rpm -ivh kernel-default-5.14.21-150400.24.21.2.x86_64.rpm --oldpackage`

To make the above kernel the default entry in the grub bootloader, run the following commands:
+
----
# grub2-set-default 'Advanced options for SLES 15-SP4>SLES 15-SP4, with Linux 5.14.21-150400.24.21-default'

# grub2-mkconfig -o /boot/grub2/grub.cfg
----

During reboot, the host will choose the above kernel as the default grub entry.
====

. Reboot the host and verify that it boots into the above-mentioned SLES15 SP4 MU kernel:
+
----
# uname -r
5.14.21-150400.24.21-default
----

. Upgrade to the latest available SLES15 SP4 `libnvme1-1.0-150400.3.12.1 & nvme-cli-2.0-150400.3.12.1` MU associated packages.
+
----
# rpm -qa|grep nvme
libnvme-devel-1.0-150400.3.12.1.x86_64
python3-libnvme-1.0-150400.3.12.1.x86_64
nvme-cli-2.0-150400.3.12.1.x86_64
libnvme1-1.0-150400.3.12.1.x86_64
nvme-cli-bash-completion-2.0-150400.3.12.1.x86_64
nvme-cli-zsh-completion-2.0-150400.3.12.1.x86_64
----

== Configure NVMe/FC

=== Broadcom/Emulex

.Steps

. Verify that you have the recommended adapter and firmware versions. For example,
+
----
# cat /sys/class/scsi_host/host*/modelname
LPe32002-M2
LPe32002-M2

# cat /sys/class/scsi_host/host*/modeldesc
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter 

# cat /sys/class/scsi_host/host*/fwrev
12.8.351.47, sli-4:2:c
12.8.351.47, sli-4:2:c

# cat /sys/module/lpfc/version
0:14.2.0.6
----


. Verify that the initiator ports are up and running:
+
----
# cat /sys/class/fc_host/host*/port_name
0x100000109b579d5e
0x100000109b579d5f
# cat /sys/class/fc_host/host*/port_state
Online
Online
----

. Verify that the NVMe/FC initiator ports are enabled and you are able to see the target ports, and all are up and running. In this example, only one initiator port is enabled and connected with two target LIFs as seen in the output:
+
----
# cat /sys/class/scsi_host/host*/nvme_info

NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x100000109b579d5e WWNN x200000109b579d5e DID x011c00 ONLINE
NVME RPORT WWPN x208400a098dfdd91 WWNN x208100a098dfdd91 DID x011503 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x208500a098dfdd91 WWNN x208100a098dfdd91 DID x010003 TARGET DISCSRVC ONLINE

NVME Statistics
LS: Xmt 0000000e49 Cmpl 0000000e49 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000003ceb594f Issue 000000003ce65dbe OutIO fffffffffffb046f
abort 00000bd2 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 000014f4 Err 00012abd

NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x100000109b579d5f WWNN x200000109b579d5f DID x011b00 ONLINE
NVME RPORT WWPN x208300a098dfdd91 WWNN x208100a098dfdd91 DID x010c03 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x208200a098dfdd91 WWNN x208100a098dfdd91 DID x012a03 TARGET DISCSRVC ONLINE

NVME Statistics
LS: Xmt 0000000e50 Cmpl 0000000e50 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000003c9859ca Issue 000000003c93515e OutIO fffffffffffaf794
abort 00000b73 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 0000159d Err 000135c3

----

. Reboot the host.

=== Enable 1MB I/O Size (Optional)

ONTAP reports an MDTS (Max Data Transfer Size) of 8 in the Identify Controller data which means the maximum I/O request size should be up to 1 MB. However, to issue I/O requests of size 1 MB for the Broadcom NVMe/FC host, the lpfc parameter `lpfc_sg_seg_cnt` should also be bumped up to 256 from the default value of 64. Use the following instructions to do so:

.Steps

. Append the value 256 in the respective `modprobe lpfc.conf` file:
+
----
# cat /etc/modprobe.d/lpfc.conf
options lpfc lpfc_sg_seg_cnt=256
----

. Run a dracut -f command, and reboot the host.

. After reboot, verify that the above setting has been applied by checking the corresponding sysfs value:
+
----
# cat /sys/module/lpfc/parameters/lpfc_sg_seg_cnt
256
----

Now the Broadcom NVMe/FC host should be able to send up 1MB I/O requests on the ONTAP namespace devices.

=== Marvell/QLogic

The native inbox qla2xxx driver included in the newer SLES15 SP4 MU kernel has the latest upstream fixes, essential for ONTAP support.

.Steps

. Verify that you are running the supported adapter driver and firmware versions, for example:
+
----
# cat /sys/class/fc_host/host*/symbolic_name
QLE2742 FW:v9.08.02 DVR:v10.02.07.800-k
QLE2742 FW:v9.08.02 DVR:v10.02.07.800-k
----

. Verify `ql2xnvmeenable` is set which enables the Marvell adapter to function as a NVMe/FC initiator:
+
----
# cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
1
----

== Configure NVMe/TCP

include::_include/nvme/reuse_configure_nvmetcp.adoc[]

.Steps

. Verify whether the initiator port can fetch the discovery log page data across the supported NVMe/TCP LIFs:
+
----
# nvme discover -t tcp -w 192.168.1.4 -a 192.168.1.31

Discovery Log Number of Records 10, Generation counter 119
=====Discovery Log Entry 0======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 0
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.37ba7d9cbfba11eba35dd039ea165514:subsystem.nvme_114_tcp_1
traddr: 192.168.2.36
sectype: none
=====Discovery Log Entry 1======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 1
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.37ba7d9cbfba11eba35dd039ea165514:subsystem.nvme_114_tcp_1
traddr: 192.168.1.31
sectype: none
=====Discovery Log Entry 2======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 0
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.37ba7d9cbfba11eba35dd039ea165514:subsystem.nvme_114_tcp_2
traddr: 192.168.2.36
sectype: none
...
----

. Verify that other NVMe/TCP initiator-target LIF combos are able to successfully fetch discovery log page data. For example,
+
----
# nvme discover -t tcp -w 192.168.1.4 -a 192.168.1.32
# nvme discover -t tcp -w 192.168.2.5 -a 192.168.2.36
# nvme discover -t tcp -w 192.168.2.5 -a 192.168.2.37
----

. Run `nvme connect-all` command across all the supported NVMe/TCP initiator-target LIFs across the nodes. Verify that you set a longer `ctrl_loss_tmo` timer retry period (for example, 30 minutes, which can be set through -l 1800) during the `connect-all` so that it retries for a longer period of time in the event of a path loss. For example,
+
----
# nvme connect-all -t tcp -w 192.168.1.4 -a 192.168.1.31 -l 1800
# nvme connect-all -t tcp -w 192.168.1.4 -a 192.168.1.32 -l 1800
# nvme connect-all -t tcp -w 192.168.2.5 -a 192.168.2.36 -l 1800
# nvme connect-all -t tcp -w 192.168.2.5 -a 192.168.2.37 -l 1800
----

== Validate NVMe-oF

.Steps

. Verify that in-kernel NVMe multipath is indeed enabled by checking:
+
----
# cat /sys/module/nvme_core/parameters/multipath
Y
----

. Verify that the appropriate NVMe-oF settings (such as, model set to NetApp ONTAP Controller and load balancing iopolicy set to round-robin) for the respective ONTAP namespaces properly reflect on the host:
+
----
# cat /sys/class/nvme-subsystem/nvme-subsys*/model
NetApp ONTAP Controller
NetApp ONTAP Controller

# cat /sys/class/nvme-subsystem/nvme-subsys*/iopolicy
round-robin
round-robin
----

. Verify that the ONTAP namespaces properly reflect on the host. For example,
+
Example (a):
+
----
# nvme list
Node                    Generic                              SN                                                        Model                                                    Namespace        Usage                           Format               FW  Rev
------------------- ---------------------------- ---------------------------------------- ---------------- -------------------------- ----------------- ------------------------  -----------------   --------
/dev/nvme1n1      /dev/ng1n1                       814vWBNRwfBGAAAAAAAB               NetApp ONTAP Controller                     1                        85.90    GB / 85.90 GB   4 KiB + 0 B         FFFFFFFF
----
+
Example (b):
+
----
# nvme list
Node                    Generic                              SN                                                        Model                                                    Namespace        Usage                           Format               FW  Rev
------------------- ---------------------------- ---------------------------------------- ---------------- -------------------------- ----------------- ------------------------  -----------------   --------
/dev/nvme0n1      /dev/ng0n1                       81CYrBQuTHQFAAAAAAAC               NetApp ONTAP Controller                     1                        85.90    GB / 85.90 GB   4 KiB + 0 B         FFFFFFFF
----

. Verify that the controller state of each path is live and has proper ANA status. For example,
+
Example (a)
----
# nvme list-subsys /dev/nvme1n1
nvme-subsys1 - NQN=nqn.1992-08.com.netapp:sn.04ba0732530911ea8e8300a098dfdd91:subsystem.nvme_145_1
\
+- nvme2 fc traddr=nn-0x208100a098dfdd91:pn-0x208200a098dfdd91,host_traddr=nn-0x200000109b579d5f:pn-0x100000109b579d5f live optimized
+- nvme3 fc traddr=nn-0x208100a098dfdd91:pn-0x208500a098dfdd91,host_traddr=nn-0x200000109b579d5e:pn-0x100000109b579d5e live optimized
+- nvme4 fc traddr=nn-0x208100a098dfdd91:pn-0x208400a098dfdd91,host_traddr=nn-0x200000109b579d5e:pn-0x100000109b579d5e live non-optimized
+- nvme6 fc traddr=nn-0x208100a098dfdd91:pn-0x208300a098dfdd91,host_traddr=nn-0x200000109b579d5f:pn-0x100000109b579d5f live non-optimized
----
+
Example (b):
+
----
#nvme list-subsys /dev/nvme0n1
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.37ba7d9cbfba11eba35dd039ea165514:subsystem.nvme_114_tcp_1
\
+- nvme0 tcp traddr=192.168.2.36 trsvcid=4420,host_traddr=192.168.1.4 live optimized
+- nvme1 tcp traddr=192.168.1.31 trsvcid=4420,host_traddr=192.168.1.4 live optimized
+- nvme10 tcp traddr=192.168.2.37 trsvcid=4420,host_traddr=192.168.1.4 live non-optimized
+- nvme11 tcp traddr=192.168.1.32 trsvcid=4420,host_traddr=192.168.1.4 live non-optimized
+- nvme20 tcp traddr=192.168.2.36 trsvcid=4420,host_traddr=192.168.2.5 live optimized
+- nvme21 tcp traddr=192.168.1.31 trsvcid=4420,host_traddr=192.168.2.5 live optimized
+- nvme30 tcp traddr=192.168.2.37 trsvcid=4420,host_traddr=192.168.2.5 live non-optimized
+- nvme31 tcp traddr=192.168.1.32 trsvcid=4420,host_traddr=192.168.2.5 live non-optimized
----

. Verify that the NetApp plug-in displays proper values for each ONTAP namespace device. For example,
+
----
# nvme netapp ontapdevices -o column
Device Vserver Namespace Path NSID UUID Size
---------------- ------------------------- -------------------------------------------------- ---- -------------------------------------- ---------
/dev/nvme1n1 vserver_fcnvme_145 /vol/fcnvme_145_vol_1_0_0/fcnvme_145_ns 1 23766b68-e261-444e-b378-2e84dbe0e5e1 85.90GB

# nvme netapp ontapdevices -o json
{
"ONTAPdevices" : [
     {
       "Device" : "/dev/nvme1n1",
       "Vserver" : "vserver_fcnvme_145",
       "Namespace_Path" : "/vol/fcnvme_145_vol_1_0_0/fcnvme_145_ns",
       "NSID" : 1,
       "UUID" : "23766b68-e261-444e-b378-2e84dbe0e5e1",
       "Size" : "85.90GB",
       "LBA_Data_Size" : 4096,
       "Namespace_Size" : 20971520
     }
  ]
}
----
+
Example (a):
+
----
# nvme netapp ontapdevices -o column
Device             Vserver                    Namespace Path                                         NSID  UUID                                                           Size
------------------- ------------------------- ---------------------------------------------------------- ------- --------------------------------------------------------- ---------
/dev/nvme0n1 vs_tcp_114              /vol/tcpnvme_114_1_0_1/tcpnvme_114_ns  1       a6aee036-e12f-4b07-8e79-4d38a9165686 85.90GB

# nvme netapp ontapdevices -o json
{
     "ONTAPdevices" : [
     {
          "Device" : "/dev/nvme0n1",
           "Vserver" : "vs_tcp_114",
          "Namespace_Path" : "/vol/tcpnvme_114_1_0_1/tcpnvme_114_ns",
          "NSID" : 1,
          "UUID" : "a6aee036-e12f-4b07-8e79-4d38a9165686",
          "Size" : "85.90GB",
          "LBA_Data_Size" : 4096,
          "Namespace_Size" : 20971520
       }      
  ]

}
----
[NOTE]
====
As seen above, there are slight changes to the 'nvme list' and 'nvme list-subsys' outputs compared to the outputs seen from previous OS releases such as SLES15 SP3:

'nvme list' now has a new column 'Generic' to denote the respective NVMe generic device.
'nvme list-subsys' now has a comma separating the trsvcid & host-traddr fields.
====

== Discovery Hardening - TP 8013 & TP 8014 (with ONTAP 9.11.1)

TP 8013 specifies the use of unique subsystem NQNs (in addition to the well-known discovery NQN) for discovery subsystems. ONTAP implements this starting 9.11.1 and the new libnmve/nvme-cli v2.x changes in SLES15 SP4 has this feature implemented and is able to connect to the specified unique subsystem NQN of the said ONTAP discovery controller.

As a part of TP 8014, ONTAP 9.11.1 has introduced two new subsystem types in the discovery log page data:

* 0x02 which indicates an NVMe I/O subsystem.
* 0x03 which indicates current discovery subsystem. 

For the Entry Flags, ONTAP sets EPCSD (Explicit Persistent Connection Support for Discovery) to 1 and 0 for Discovery subsystems and I/O subsystems respectively, whereas it also sets DUPRETINFO (Duplicate Returned Information) to 1 and 0 for Discovery subsystems and I/O subsystems respectively. The new libnvme/nvme-cli v2.x changes in SLES15 SP4 is able to consume this properly and connect accordingly. 

Refer to the following procedure:

.Steps

. Verify that the new libnvme/nvme-cli v2.x changes in SLES15 SP4 have been implemented successfully:
+
----
# nvme discover -t tcp -w 192.168.1.16 -a 192.168.1.116

Discovery Log Number of Records 8, Generation counter 18
=====Discovery Log Entry 0======
trtype: tcp
adrfam: ipv4
subtype: current discovery subsystem
treq: not specified
portid: 0
trsvcid: 8009
subnqn: nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:discovery
traddr: 192.168.2.117
eflags: explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 1======
trtype: tcp
adrfam: ipv4
subtype: current discovery subsystem
treq: not specified
portid: 1
trsvcid: 8009
subnqn: nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:discovery
traddr: 192.168.1.117
eflags: explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 2======
trtype: tcp
adrfam: ipv4
subtype: current discovery subsystem
treq: not specified
portid: 2
trsvcid: 8009
subnqn: nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:discovery
traddr: 192.168.2.116
eflags: explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 3======
trtype: tcp
adrfam: ipv4
subtype: current discovery subsystem
treq: not specified
portid: 3
trsvcid: 8009
subnqn: nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:discovery
traddr: 192.168.1.116
eflags: explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 4======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 0
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:subsystem.subsys_CLIENT116
traddr: 192.168.2.117
eflags: not specified
sectype: none
=====Discovery Log Entry 5======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 1
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:subsystem.subsys_CLIENT116
traddr: 192.168.1.117
eflags: not specified
sectype: none
=====Discovery Log Entry 6======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 2
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:subsystem.subsys_CLIENT116
traddr: 192.168.2.116
eflags: not specified
sectype: none
=====Discovery Log Entry 7======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 3
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:subsystem.subsys_CLIENT116
traddr: 192.168.1.116
eflags: not specified
sectype: none
----

. Create a Persistent Discovery Controller (PDC) connection to this discovery subsystem:
+
`# nvme discover -t tcp -w 192.168.1.16 -a 192.168.1.116 -p`

. Validate that the above PDC was created by connecting to its unique subsystem NQN itself (and not the well-known NQN) in ONTAP:
+
----
*> vserver nvme show-discovery-controller -instance -vserver vs_CLIENT116

Vserver Name: vs_CLIENT116
Controller ID: 00C0h
Discovery Subsystem NQN: nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:discovery
Logical Interface UUID: d23cbb0a-c0a6-11ec-9731-d039ea165abc
Logical Interface: CLIENT116_lif_4a_1
Node: A400-14-124
Host NQN: nqn.2014-08.org.nvmexpress:uuid:12372496-59c4-4d1b-be09-74362c0c1afc
Transport Protocol: nvme-tcp
Initiator Transport Address: 192.168.1.16
Host Identifier: 59de25be738348f08a79df4bce9573f3
Admin Queue Depth: 32
Header Digest Enabled: false
Data Digest Enabled: false
Vserver UUID: 48391d66-c0a6-11ec-aaa5-d039ea165514
----

== In-band Auth - TP 8006 (with ONTAP 9.12.1)

In-band authentication enables NVMe hosts and NVMe controllers to authenticate each other when an NVMe over Fabrics connection is established. DH-HMAC-CHAP is the authentication protocol that is currently specified to be used for NVMe over Fabrics. DH-HMAC-CHAP provides bidirectional or unidirectional authentication between a host and a controller. This authentication protocol requires each host or controller to be associated with a DH-HMAC-CHAP key. The key for a host or controller is derived from an administratively configured secret and the NQN of the NVMe host or the controller respectively. An NVMe host or controller that seeks to authenticate its peer is required to know the key associated with its peer.

.Steps

. Run the following command to generate the dhchap keys on the SLES15 SP4 host :
+
`Usage: nvme gen-dhchap-key <device> [OPTIONS]`

. Generate a DH-HMAC-CHAP host key usable for NVMe In-Band Authentication.
+
----
Options:
[ --secret=<STRING>, -s <STRING> ] --- Optional secret (in hexadecimal characters) to be used to
initialize the host key.
[ --key-length=<NUM>, -l <NUM> ] --- Length of the resulting key (32, 48, or 64 bytes).
[ --nqn=<STRING>, -n <STRING> ] --- Host NQN to use for key transformation.
[ --hmac=<NUM>, -m <NUM> ] --- HMAC function to use for key
transformation (0 = none, 1 = SHA-256, 2 = SHA-384, 3 = SHA-512).
----
+
For example, to generate a random dhchap-key using hmac set to 3 (i.e. SHA-512) along with the host's nqn:
+
----
# nvme gen-dhchap-key -m 3 -n nqn.2014-08.org.nvmexpress:uuid:d3ca725a-ac8d-4d88-b46a-174ac235139b
DHHC-1:03:J2UJQfj9f0pLnpF/ASDJRTyILKJRr5CougGpGdQSysPrLu6RW1fGl5VSjbeDF1n1DEh3nVBe19nQ/LxreSBeH/bx/pU=:
----
+
This may be used as the secret on the host and the target for a given subsystem during the nvme connect with the following dhchap options:
+
----
[ --dhchap-secret=<STR>, -S <STR> ] --- user-defined dhchap key (if default not used)
[ --dhchap-ctrl-secret=<STR>, -C <STR> ] --- user-defined dhchap controller key (for bi-directional authentication)
----

If no secret is specified in the Linux nvme-cli dhchap option during the connect, it would attempt to locate the default key at /etc/nvme/hostkey. If not found, no authentication is attempted.

=== In-band Auth using CLI

Following procedure is an example of using In-band Auth using bidirectional connect through the CLI.

.Steps

. Generate the host and controller keys using the nvme gen-dhchap-key:
+
----
# cat /etc/nvme/hostnqn
nqn.2014-08.org.nvmexpress:uuid:e58eca24-faff-11ea-8fee-3a68dd3b5c5f
----
+
----
# nvme gen-dhchap-key -m 1 -n nqn.2014-08.org.nvmexpress:uuid:e58eca24-faff-11ea-8fee-3a68dd3b5c5f
DHHC-1:01:NunEWY7AZlXqxITGheByarwZdQvU4ebZg9HOjIr6nOHEkxJg:
----
+
----
# nvme gen-dhchap-key -m 3 -n nqn.2014-08.org.nvmexpress:uuid:e58eca24-faff-11ea-8fee-3a68dd3b5c5f
DHHC-1:03:2YJinsxa2v3+m8qqCiTnmgBZoH6mIT6G/6f0aGO8viVZB4VLNLH4z8CvK7pVYxN6S5fOAtaU3DNi12rieRMfdbg3704=:
----

. Specify both these host and controller keys during 'subsystem host add' in ONTAP:
+
----
::> nvme subsystem host add -vserver integ -subsystem subsys114 -host-nqn nqn.2014-08.org.nvmexpress:uuid:e58eca24-faff-11ea-8fee-3a68dd3b5c5f -dhchap-dh-group 3072-bit -dhchap-host-secret-key DHHC-1:01:NunEWY7AZlXqxITGheByarwZdQvU4ebZg9HOjIr6nOHEkxJg: -dhchap-controller-secret-key DHHC-1:03:2YJinsxa2v3+m8qqCiTnmgBZoH6mIT6G/6f0aGO8viVZB4VLNLH4z8CvK7pVYxN6S5fOAtaU3DNi12rieRMfdbg3704=:
(vserver nvme subsystem host add)
----

. Now on the host, proceed with a bidirectional connect specifying both these keys:
+
----
# nvme connect -t tcp -w 192.168.1.14 -a 192.168.1.201 -n nqn.1992-08.com.netapp:sn.752d9843d2ab11ecbbcbd039ea165514:subsystem.subsys114 -S DHHC-1:01:NunEWY7AZlXqxITGheByarwZdQvU4ebZg9HOjIr6nOHEkxJg: -C DHHC-1:03:2YJinsxa2v3+m8qqCiTnmgBZoH6mIT6G/6f0aGO8viVZB4VLNLH4z8CvK7pVYxN6S5fOAtaU3DNi12rieRMfdbg3704=:
----

. Verify that the bidirectional connect is successful and the keys are properly registered in the host sysfs:
+
----
# nvme list
Node Generic SN Model Namespace Usage Format FW Rev
--------------------- --------------------- -------------------- -----------------------------------
/dev/nvme0n1 /dev/ng0n1 81CYrBSoKGYrAAAAAAAN NetApp ONTAP Controller 1 1.07 GB / 1.07 GB 4 KiB + 0 B FFFFFFFF

# nvme list-subsys
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.752d9843d2ab11ecbbcbd039ea165514:subsystem.subsys114
\
+- nvme0 tcp traddr=192.168.1.201,trsvcid=4420,host_traddr=192.168.1.14 live

# nvme list-subsys /dev/nvme0n1
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.752d9843d2ab11ecbbcbd039ea165514:subsystem.subsys114
\
+- nvme0 tcp traddr=192.168.1.201,trsvcid=4420,host_traddr=192.168.1.14 live optimized

# cat /sys/class/nvme-subsystem/nvme-subsys0/nvme0/dhchap_secret
DHHC-1:01:NunEWY7AZlXqxITGheByarwZdQvU4ebZg9HOjIr6nOHEkxJg:

# cat /sys/class/nvme-subsystem/nvme-subsys0/nvme0/dhchap_ctrl_secret
DHHC-1:03:2YJinsxa2v3+m8qqCiTnmgBZoH6mIT6G/6f0aGO8viVZB4VLNLH4z8CvK7pVYxN6S5fOAtaU3DNi12rieRMfdbg3704=:
----

=== In-band Auth using config JSON file

If you need to specify different keys for different subsystems during a nvme connect-all, then the only option to do that would be run the connect-all command using the `-J <cfg file>` option and specifying different keys for respective subsystems. Note that `dhchap_key` corresponds to `dhchap_secret` and `dhchap_ctrl_key` corresponds to `dhchap_ctrl_secret`. Refer to the folloiwng procedure:

.Steps

. Run the following command:
+
----
# cat /etc/nvme/config.json
[
  {
        "hostnqn":"nqn.2014-08.org.nvmexpress:uuid:12372496-59c4-4d1b-be09-74362c0c1afc",
        "hostid":"3ae10b42-21af-48ce-a40b-cfb5bad81839",
        "dhchap_key":"DHHC-1:03:Cu3ZZfIz1WMlqZFnCMqpAgn/T6EVOcIFHez215U+Pow8jTgBF2UbNk3DK4wfk2EptWpna1rpwG5CndpOgxpRxh9m41w=:"
   },
   {
         "hostnqn":"nqn.2014-08.org.nvmexpress:uuid:12372496-59c4-4d1b-be09-74362c0c1afc",
         "subsystems":[
         {
              "nqn":"nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:subsystem.subsys_CLIENT116",
              "ports":[
              {
                        "transport":"tcp",
                        "traddr":"192.168.1.117",
                        "host_traddr":"192.168.1.16",
                        "trsvcid":"4420",
                        "dhchap_ctrl_key":"DHHC-1:01:0h58bcT/uu0rCpGsDYU6ZHZvRuVqsYKuBRS0Nu0VPx5HEwaZ:"
               },
              {
                        "transport":"tcp",
                        "traddr":"192.168.1.116",
                        "host_traddr":"192.168.1.16",
                        "trsvcid":"4420",
                        "dhchap_ctrl_key":"DHHC-1:01:0h58bcT/uu0rCpGsDYU6ZHZvRuVqsYKuBRS0Nu0VPx5HEwaZ:"
               },
               {
                         "transport":"tcp",
                         "traddr":"192.168.2.117",
                         "host_traddr":"192.168.2.16",
                         "trsvcid":"4420",
                         "dhchap_ctrl_key":"DHHC-1:01:0h58bcT/uu0rCpGsDYU6ZHZvRuVqsYKuBRS0Nu0VPx5HEwaZ:"
                },
                {
                           "transport":"tcp",
                           "traddr":"192.168.2.116",
                           "host_traddr":"192.168.2.16",
                           "trsvcid":"4420",
                           "dhchap_ctrl_key":"DHHC-1:01:0h58bcT/uu0rCpGsDYU6ZHZvRuVqsYKuBRS0Nu0VPx5HEwaZ:"
                 }
              ]
          }
       ]
   }
]
----

. Now run a connect-all to this config JSON file (will automatically pick up the discovery options from the /etc/nvme/discovery.conf):
+
----
# nvme connect-all -J /etc/nvme/config.json
traddr=192.168.2.116 is already connected
traddr=192.168.1.116 is already connected
traddr=192.168.2.117 is already connected
traddr=192.168.1.117 is already connected
traddr=192.168.2.117 is already connected
traddr=192.168.1.117 is already connected
traddr=192.168.2.116 is already connected
traddr=192.168.1.116 is already connected
traddr=192.168.2.116 is already connected
traddr=192.168.1.116 is already connected
traddr=192.168.2.117 is already connected
traddr=192.168.1.117 is already connected
----
+
----
# nvme list
Node         Generic    SN  
--------------------- --------------------- 
/dev/nvme1n2 /dev/ng1n2 81CYrBSoKGRJAAAAAAAB 
/dev/nvme1n1 /dev/ng1n1 81CYrBSoKGRJAAAAAAAB 

Model                   Namespace Usage            Format  FW Rev
---------------------------------------------------------------
NetApp ONTAP Controller 2         5.37 GB / 5.37 GB 4 KiB + 0 B FFFFFFFF
NetApp ONTAP Controller 1         5.37 GB / 5.37 GB 4 KiB + 0 B FFFFFFFF
----
+
----
# nvme list-subsys
nvme-subsys1 - NQN=nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:subsystem.subsys_CLIENT116
\
+- nvme1 tcp traddr=192.168.2.117,trsvcid=4420,host_traddr=192.168.2.16 live
+- nvme2 tcp traddr=192.168.1.117,trsvcid=4420,host_traddr=192.168.1.16 live
+- nvme3 tcp traddr=192.168.2.116,trsvcid=4420,host_traddr=192.168.2.16 live
+- nvme4 tcp traddr=192.168.1.116,trsvcid=4420,host_traddr=192.168.1.16 live
----
+
----
# nvme list-subsys /dev/nvme1n1
nvme-subsys1 - NQN=nqn.1992-08.com.netapp:sn.48391d66c0a611ecaaa5d039ea165514:subsystem.subsys_CLIENT116
\
+- nvme1 tcp traddr=192.168.2.117,trsvcid=4420,host_traddr=192.168.2.16 live non-optimized
+- nvme2 tcp traddr=192.168.1.117,trsvcid=4420,host_traddr=192.168.1.16 live non-optimized
+- nvme3 tcp traddr=192.168.2.116,trsvcid=4420,host_traddr=192.168.2.16 live optimized
+- nvme4 tcp traddr=192.168.1.116,trsvcid=4420,host_traddr=192.168.1.16 live optimized
----
+
----
# cat /sys/class/nvme-subsystem/nvme-subsys1/nvme1/dhchap_secret
DHHC-1:03:Cu3ZZfIz1WMlqZFnCMqpAgn/T6EVOcIFHez215U+Pow8jTgBF2UbNk3DK4wfk2EptWpna1rpwG5CndpOgxpRxh9m41w=:

# cat /sys/class/nvme-subsystem/nvme-subsys1/nvme1/dhchap_ctrl_secret
DHHC-1:01:0h58bcT/uu0rCpGsDYU6ZHZvRuVqsYKuBRS0Nu0VPx5HEwaZ:

# cat /sys/class/nvme-subsystem/nvme-subsys1/nvme2/dhchap_secret
DHHC-1:03:Cu3ZZfIz1WMlqZFnCMqpAgn/T6EVOcIFHez215U+Pow8jTgBF2UbNk3DK4wfk2EptWpna1rpwG5CndpOgxpRxh9m41w=:

# cat /sys/class/nvme-subsystem/nvme-subsys1/nvme2/dhchap_ctrl_secret
DHHC-1:01:0h58bcT/uu0rCpGsDYU6ZHZvRuVqsYKuBRS0Nu0VPx5HEwaZ:

# cat /sys/class/nvme-subsystem/nvme-subsys1/nvme3/dhchap_secret
DHHC-1:03:Cu3ZZfIz1WMlqZFnCMqpAgn/T6EVOcIFHez215U+Pow8jTgBF2UbNk3DK4wfk2EptWpna1rpwG5CndpOgxpRxh9m41w=:

# cat /sys/class/nvme-subsystem/nvme-subsys1/nvme3/dhchap_ctrl_secret
DHHC-1:01:0h58bcT/uu0rCpGsDYU6ZHZvRuVqsYKuBRS0Nu0VPx5HEwaZ:

# cat /sys/class/nvme-subsystem/nvme-subsys1/nvme4/dhchap_secret
DHHC-1:03:Cu3ZZfIz1WMlqZFnCMqpAgn/T6EVOcIFHez215U+Pow8jTgBF2UbNk3DK4wfk2EptWpna1rpwG5CndpOgxpRxh9m41w=:

# cat /sys/class/nvme-subsystem/nvme-subsys1/nvme4/dhchap_ctrl_secret
DHHC-1:01:0h58bcT/uu0rCpGsDYU6ZHZvRuVqsYKuBRS0Nu0VPx5HEwaZ:
----

== Troubleshooting

=== LPFC verbose logging

include::_include/nvme/reuse_nvme_verbose_logging.adoc[]

=== Qla2xxx verbose logging

include::_include/nvme/reuse_nvme_qla2xxx_verbose_logging.adoc[]



include::_include/hu/reuse_hu_common_nvme_cli_errors.adoc[]

=== When to contact technical support

include::_include/hu/reuse_hu_contact_tech_support.adoc[]

== Known issues and workarounds

[cols="10,30,10",options="header"]
|===
|NetApp Bug ID	|Title	|Workaround
|1490498	| Host crash at crypto_ahash_digest() during NVMe reauth | None
|1496739	| Host fails to disconnect after sending auth_failure2 during re-auth	| None	
|===

//BURT 1529116 30 Jan 2023


