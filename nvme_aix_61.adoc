---
sidebar: sidebar
permalink: nvme_aix_61.html
keywords: nvme, linux, rhel, red hat, enterprise, aix 
summary: How to Configure NVMe/FC Host for AIX with ONTAP
---

= NVMe/FC Host Configuration for AIX with ONTAP
:toc: macro
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
You can enable NVMe-oF on IBM AIX and VIOS/PowerVM hosts using ONTAP storage as the target.

For additional details on the supported configurations, see the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix Tool^].

== Features

*	Beginning with ONTAP 9.13.1, NVMe over Fibre Channel (NVMe/FC) support is added for IBM  AIX 7.2 TL5 SP6, AIX 7.3 TL1 SP2 and VIOS 3.1.4.2 releases with SAN boot support.
*	You can run NVMe and SCSI traffic on the same co-existent host. This is a commonly deployed host configuration.
*	No separate PCM (Path Control Module, such as HUK for AIX SCSI MPIO support) is required for the NVMe devices.
*	NVMe/FC is supported only with Power9 and Power10 IBM servers. 
*	Virtualization support with NetApp (VIOS/PowerVM) is introduced with VIOS 3.1.4.21 version. This is supported through NPIV storage virtualization mode only using Power10 IBM server.

== Limitations

* NVMe/FC is not supported for Qlogic/Marvel 32G FC HBAs.
*	SAN boot is not supported for NVMe/FC devices using Power9 IBM server.


== IBM NVMe/FC support prerequisites
Initial NVMe/FC support is only provided with 32Gb FC Emulex adapters (EN1A, EN1B, EN1L, EN1M), and 64Gb FC adapters (EN1N, EN1P) which have at least the 12.4.257.30 (FCode level 12.8.9) adapter firmware level

By default, the ANATT (Asymmetric Namespace Access Transition Timeout) for the AIX host OS is 30s. However, for all ONTAP workflows to be non disruptive, we would require the ANATT value as 60s.

IBM is planning an iFix which will cap the ANATT at 60s (as opposed to the default 30s right now in GA). This will be available as a PMR (Problem Management Record) and tracked via TS012877410). Once this is made available, NetApp support will be called out in the IMT

For NVMe/FC AIX support, the Customers are required to download and install the iFix from IBM website on the GA versions of AIX OS versions. This is not required for the VIOS/PowerVM OS

For non-virtualised mode (HBA assigned to the AIX LPAR or the physical stack)


== NVMe host multipath

IBM MPIO is used for NVMe multipathing which is provided by default when you install the operating system (OS).

=== Verify NVMe multipathing

IBM lsmpio is a software tool used to improve the reliability and availability of storage in IBM AIX environments. It enables multiple paths to a storage device and provides load balancing across these paths. The tool monitors the health and availability of paths and automatically routes I/O requests to the available paths. It also supports dynamic configuration changes and failover mechanisms to prevent data loss in case of path failure. IBM lsmpio is commonly used in high-performance computing and mission-critical applications to ensure optimal storage performance and availability.

.Steps

. Verify NVME multipathing by using the "lsmpio" command (Load Source Multi-Pathing for IBM AIX):

----
#[root@aix_server /]: lsmpio -l hdisk1
----
+
*Example output:*
+
----
name     path_id  status   path_status  parent  connection
hdisk1  8         Enabled  Sel,Opt       nvme12  fcnvme0, 9
hdisk1  9         Enabled  Sel,Non      nvme65  fcnvme1, 9
hdisk1  10       Enabled  Sel,Opt       nvme37  fcnvme1, 9
hdisk1  11        Enabled  Sel,Non      nvme60  fcnvme0, 9
----

== Configure NVMe/FC

Broadcom/Emulex

.Steps

. Verify that you are using the supported adapter. For the most current list of supported adapter driver and firmware versions, see the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix Tool^].
+
----
$ lsmap -all -npiv
----
+
*Example output*:
+
----
Name          Physloc                            ClntID ClntName       ClntOS
------------- ---------------------------------- ------ -------------- -------
vfchost0      U9105.22A.785DB61-V2-C2                 4 s1022-iop-mcc- AIX

Status:LOGGED_IN
FC name:fcs4                    FC loc code:U78DA.ND0.WZS01UY-P0-C7-T0
Ports logged in:3
Flags:0xea<LOGGED_IN,STRIP_MERGE,SCSI_CLIENT,NVME_CLIENT>
VFC client name:fcs0            VFC client DRC:U9105.22A.785DB61-V4-C2

Name          Physloc                            ClntID ClntName       ClntOS
------------- ---------------------------------- ------ -------------- -------
vfchost1      U9105.22A.785DB61-V2-C3                 4

Status:NOT_LOGGED_IN
FC name:                        FC loc code:
Ports logged in:0
Flags:0x81<NOT_MAPPED,NOT_CONNECTED>
VFC client name:                VFC client DRC:
----

. By default, the NVMeoF protocol support is enabled in the physical Fibre Channel but disabled in the Virtual Fibre Channel (VFC) on Virtual I/O Server (VIOS). You can run ioscli vfcctrl command on the VIOS to enable the NPIV-NVMeoF protocol support. Decide whether you want to enable NVMe for all current and future adapters

** To enable NVMe for all adapters, complete the following steps:
.. Change the dflt_enabl_nvme attribute value of viosnpiv0 pseudo device to yes.
.. Set the enable_nvme attribute value to yes for all the VFC host devices.

** If you want to enable NVMeoF protocol only on a few adapters, then you can change the enable_nvme value of VFC host device attribute to yes.

+
----
$  vfcctrl -enable -protocol nvme -vadapter vfchost0
The "nvme" protocol for "vfchost0" is enabled.

# lsattr -El vfchost0
alt_site_wwpn       WWPN to use - Only set after migration   False
current_wwpn  0     WWPN to use - Only set after migration   False
enable_nvme   yes   Enable or disable NVME protocol for NPIV True
label               User defined label                       True
limit_intr    false Limit NPIV Interrupt Sources             True
map_port      fcs4  Physical FC Port                         False
num_per_nvme  0     Number of NPIV NVME queues per range     True
num_per_range 0     Number of NPIV SCSI queues per range     True


# chdev -l viosnpiv0 -a dflt_enabl_nvme=yes

# lsattr -El viosnpiv0
bufs_per_cmd    10  NPIV Number of local bufs per cmd                    True
dflt_enabl_nvme yes Default NVME Protocol setting for a new NPIV adapter True
num_local_cmds  5   NPIV Number of local cmds per channel                True
num_per_nvme    8   NPIV Number of NVME queues per range                 True
num_per_range   8   NPIV Number of SCSI queues per range                 True
secure_va_info  no  NPIV Secure Virtual Adapter Information              True
----


. Check "FC-NVMe Protocol Device" is created on the server:
+
----
# [root@aix_server /]: lsdev |grep fcnvme
----
+
*Example output*
+
----
fcnvme0       Available 00-00-02    FC-NVMe Protocol Device
fcnvme1       Available 00-01-02    FC-NVMe Protocol Device
----

. Record the host NQN from the server:
+
----
# [root@aix_server /]: lsattr -El fcnvme0
----
+
*Example output*
+
----
attach     switch                                                               How this adapter is connected  False
autoconfig available                                                            Configuration State            True
host_nqn   nqn.2014-08.org.nvmexpress:uuid:64e039bd-27d2-421c-858d-8a378dec31e8 Host NQN (NVMe Qualified Name) True
----
+
----
[root@aix_server /]: lsattr -El fcnvme1
attach     switch                                                               How this adapter is connected  False
autoconfig available                                                            Configuration State            True
host_nqn   nqn.2014-08.org.nvmexpress:uuid:64e039bd-27d2-421c-858d-8a378dec31e8 Host NQN (NVMe Qualified Name) True
[root@aix_server /]:
----


[root@aix_server /]: lsattr -El sys0 -a partition_uuid
partition_uuid 64e039bd-27d2-421c-858d-8a378dec31e8 Partition UUID False
[root@aix_server /]:
----

. Check the host NQN and verify that it matches the host NQN string for the corresponding subsystem on the ONTAP array. For e.g.
+
----
::> vserver nvme subsystem host show -vserver vs_s922-55-lpar2
----
+
*Example output*
+
----
Vserver          Subsystem                Host NQN
------- --------- ----------------------------------------------------------
vs_s922-55-lpar2 subsystem_s922-55-lpar2  nqn.2014-08.org.nvmexpress:uuid:64e039bd-27d2-421c-858d-8a378dec31e8
----

. Verify that the initiator ports are up and running, and able to see the target LIFs.


== Validate NVMe-oF

You can use the following procedure to validate NVMe-oF.

.Steps

. Verify that the ONTAP namespaces correctly reflect on the host. 
+
----
# [root@aix_server /]: lsdev -Cc disk |grep NVMe
----
+
*Example output*
+
----
hdisk1  Available 00-00-02 NVMe 4K Disk



      #[root@aix_server /]: lsmpio -l hdisk1
      name     path_id  status   path_status  parent  connection

      hdisk1  8        Enabled  Sel,Opt      nvme12  fcnvme0, 9
      hdisk1  9        Enabled  Sel,Non      nvme65  fcnvme1, 9
      hdisk1  10       Enabled  Sel,Opt      nvme37  fcnvme1, 9
      hdisk1  11       Enabled  Sel,Non      nvme60  fcnvme0, 9
----

== SAN booting

SAN booting is supported with both physical and virtual stacks. Refer to the IBM documentation to set up SAN boot.


== Known issues 
       
[cols="10,30,30,10",options="header"]
|===
|Burt ID |Title |Description
|1536439|AIX NVMe/FC rescan results in redundant controllers during LIF toggles|
|1553249|AIX NVMe/FC default APD time to be modified for supporting MCC Unplanned Switchover events|
|1546017|AIX NVMe/FC caps ANATT at 60s, instead of 120s as advertised by ONTAP|
|1541386|AIX NVMe/FC hits EIO after ANATT expiry|
|1541380|AIX NVMe/FC waits for half/full ANATT to expire before resuming I/O after ANA AEN|
|===

== Troubleshooting

Before troubleshooting any NVMe/FC failures, verify that you are running a configuration that is compliant to the IMT specifications and then proceed with the next steps to debug any host side issues.

=== Enable verbose logging

If you have an issue with your configuration, verbose logging can provide essential information for troubleshooting.

.Steps

The procedure to set verbose logging for Qlogic (Qla2xxx) is different from the procedure to set LPFC verbose logging.

[role="tabbed-block"]
====

.LPFC
--

include::_include/nvme/reuse_nvme_verbose_logging.adoc[]
--

.Qla2xxx 
--
include::_include/nvme/reuse_nvme_qla2xxx_verbose_logging.adoc[]
--
====

include::_include/hu/reuse_hu_common_nvme_cli_errors.adoc[]

=== When to contact technical support

include::_include/hu/reuse_hu_contact_tech_support.adoc[]
