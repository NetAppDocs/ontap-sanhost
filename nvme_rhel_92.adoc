---
sidebar: sidebar
permalink: nvme_rhel_92.html
keywords: nvme, linux, rhel, red hat, enterprise
summary: How to Configure NVMe-oF Host for RHEL 8.7 with ONTAP
---

= NVMe-oF Host Configuration for RHEL 9.2 with ONTAP
:toc: macro
:hardbreaks:
:toclevels: 1
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/
:source-highlighter: highlighter.js

[.lead]

NVMe over Fabrics or NVMe-oF (including NVMe/FC and NVMe/TCP) is supported with RHEL 9.1 with Asymmetric Namespace Access (ANA) that is required for surviving storage failovers (SFOs) on the ONTAP array. ANA is the asymmetric logical unit access (ALUA) equivalent in the NVMe-oF environment, and is currently implemented with in-kernel NVMe Multipath. This document contains the details for enabling NVMe-oF with in-kernel NVMe Multipath using ANA on RHEL 9.1 and ONTAP as the target.

Refer to the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix^] for accurate details regarding supported configurations.

== Features

*	RHEL 9.2 includes support for NVMe/TCP in addition to NVMe/FC. The NetApp plugin in the native `nvme-cli` package can display ONTAP details for both NVMe/FC and NVMe/TCP namespaces.

*	RHEL 9.2 includes support for in-kernel NVMe multipath for NVMe namespaces enabled by default, without the need for explicit settings.

*	RHEL 9.2 supports use of NVMe and SCSI co-existent traffic on the same host on a given HBA adapter, without the explicit `dm-multipath` settings to prevent claiming NVMe namespaces.

== Validate software versions

You can validate the minimum supported RHEL 9.2 software versions using the following procedure.

.Steps

. Install RHEL 9.2 on the server. After the installation is complete, verify that you are running the specified OL 9.0 GA kernel. 
+
----
# uname -r
----
+
*Example output:*
+
----
5.15.0-0.30.19.el9uek.x86_64
----

. Install the `nvme-cli` package:
+
----
# rpm -qa|grep nvme-cli
----
+
*Example output:*
+
----
nvme-cli-1.16-3.el9.x86_64
----

. On the host, verify that the `hostnqn` string matches the `hostnqn` string for the corresponding subsystem on the ONTAP array:
+
----
# cat /etc/nvme/hostnqn
nqn.2014-08.org.nvmexpress:uuid:325e7554-1f9b-11ec-8489-3a68dd61a4df
----
+
----
::> vserver nvme subsystem host show -vserver vs_ol_nvme
----
+
*Example output:*
+
----
Vserver     Subsystem          Host NQN
----------- --------------- ----------------------------------------------------------
vs_nvme207   rhel_207_LPe32002    nqn.2014-08.org.nvmexpress:uuid:325e7554-1f9b-11ec-8489-3a68dd61a4df
----
+
[NOTE]
If the `hostnqn` strings do not match, you should use the `vserver modify` command to update the `hostnqn` string on your corresponding ONTAP array subsystem to match the `hostnqn` string from `/etc/nvme/hostnqn` on the host.


== Configure NVMe/FC

You can configure NVME/FC for Broadcom/Emulex adapters or Marvell/Qlogic adapters.

[role="tabbed-block"]
====
.Broadcom/Emulex
--
.Steps

. Verify that you are using the supported adapter model. 
+
----
# cat /sys/class/scsi_host/host*/modelname
----
+
*Example output:*
+
----
LPe32002-M2
LPe32002-M2
----
+
----
# cat /sys/class/scsi_host/host*/modeldesc
----
+
*Example output:*
+
----
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
----

. Verify that you are using the recommended Broadcom `lpfc` firmware and inbox driver. For the most current list of supported adapter driver and firmware versions, see the link:https://mysupport.netapp.com/matrix/[NetApp Interoperability Matrix^]:
+
----
# cat /sys/class/scsi_host/host*/fwrev
14.0.505.11, sli-4:2:c
14.0.505.11, sli-4:2:c

# cat /sys/module/lpfc/version
0:12.8.0.11
----

. Verify that `lpfc_enable_fc4_type` is set to `3`:
+
----
# cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
3
----

. Verify that the initiator ports are up and running, and that you can see the target LIFs:
+
----
# cat /sys/class/fc_host/host*/port_name
0x100000109b1c1204
0x100000109b1c1205
# cat /sys/class/fc_host/host*/port_state
Online
Online
# cat /sys/class/scsi_host/host*/nvme_info
NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x100000109b1c1204 WWNN x200000109b1c1204 DID x011d00 ONLINE
NVME RPORT WWPN x203800a098dfdd91 WWNN x203700a098dfdd91 DID x010c07 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x203900a098dfdd91 WWNN x203700a098dfdd91 DID x011507 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000f78 Cmpl 0000000f78 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000002fe29bba Issue 000000002fe29bc4 OutIO 000000000000000a
abort 00001bc7 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00001e15 Err 0000d906
NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x100000109b1c1205 WWNN x200000109b1c1205 DID x011900 ONLINE
NVME RPORT WWPN x203d00a098dfdd91 WWNN x203700a098dfdd91 DID x010007 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x203a00a098dfdd91 WWNN x203700a098dfdd91 DID x012a07 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000fa8 Cmpl 0000000fa8 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000002e14f170 Issue 000000002e14f17a OutIO 000000000000000a
abort 000016bb noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00001f50 Err 0000d9f8
----
--

.Marvell/QLogic FC Adapter for NVMe/FC
--
.Steps

. Verify that you are running the supported adapter driver and firmware versions. The native inbox qla2xxx driver included in the OL 9.0 GA kernel has the latest upstream fixes essential for ONTAP support:
+
----
# cat /sys/class/fc_host/host*/symbolic_name
QLE2742 FW:v9.08.02 DVR:v10.02.00.106-k
QLE2742 FW:v9.08.02 DVR:v10.02.00.106-k
----

. Verify that `ql2xnvmeenable` is set which enables the Marvell adapter to function as an NVMe/FC initiator:
+
----
# cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
1
----
--
====

== Configure NVMe/TCP

include::_include/nvme/reuse_configure_nvmetcp.adoc[]

.Steps

. Verify that the initiator port can fetch the discovery log page data across the supported NVMe/TCP LIFs:
+
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
*Example output:*
+
----
#  nvme discover -t tcp -w 192.168.6.13 -a 192.168.6.15
Discovery Log Number of Records 6, Generation counter 8
=====Discovery Log Entry 0======
trtype: tcp
adrfam: ipv4
subtype: unrecognized
treq: not specified
portid: 0
trsvcid: 8009
subnqn: nqn.1992-08.com.netapp:sn.1c6ac66338e711eda41dd039ea3ad566:discovery
traddr: 192.168.6.17
sectype: none
=====Discovery Log Entry 1======
trtype: tcp
adrfam: ipv4
subtype: unrecognized
treq: not specified
portid: 1
trsvcid: 8009
subnqn: nqn.1992-08.com.netapp:sn.1c6ac66338e711eda41dd039ea3ad566:discovery
traddr: 192.168.5.17
sectype: none
=====Discovery Log Entry 2======
trtype: tcp
adrfam: ipv4
subtype: unrecognized
treq: not specified
portid: 2
trsvcid: 8009
subnqn: nqn.1992-08.com.netapp:sn.1c6ac66338e711eda41dd039ea3ad566:discovery
traddr: 192.168.6.15
sectype: none
=====Discovery Log Entry 3======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 0
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.1c6ac66338e711eda41dd039ea3ad566:subsystem.host_95
traddr: 192.168.6.17
sectype: none
..........

----

. Verify that the other NVMe/TCP initiator-target LIF combinations are able to successfully fetch discovery log page data. 
+
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
*Example output:*
+
----
# nvme discover -t tcp -w 192.168.5.13 -a 192.168.5.15
# nvme discover -t tcp -w 192.168.5.13 -a 192.168.5.17
# nvme discover -t tcp -w 192.168.6.13 -a 192.168.6.15
# nvme discover -t tcp -w 192.168.6.13 -a 192.168.6.17
----

. Run the `nvme connect-all` command across all the supported NVMe/TCP initiator-target LIFs across the nodes, and set the controller loss timeout period for at least 30 minutes or 1800 seconds:
+
----
nvme connect-all -t tcp -w host-traddr -a traddr -l 1800
----
+
*Example output:*
+
----
# nvme connect-all -t tcp -w 192.168.5.13 -a 192.168.5.15 -l 1800
# nvme connect-all -t tcp -w 192.168.5.13 -a 192.168.5.17 -l 1800
# nvme connect-all -t tcp -w 192.168.6.13 -a 192.168.6.15 -l 1800
# nvme connect-all -t tcp -w 192.168.6.13 -a 192.168.6.17 -l 1800
----

===	Enable 1MB I/O size

include::_include/nvme/reuse_nvme_enabling_broadcom_1mb_size.adoc[]


== Validate NVMe-oF

.Steps

. Verify the following NVMe/FC settings on the OL 9.0 host:
+
----
# cat /sys/module/nvme_core/parameters/multipath
Y
# cat /sys/class/nvme-subsystem/nvme-subsys*/model
NetApp ONTAP Controller
NetApp ONTAP Controller
# cat /sys/class/nvme-subsystem/nvme-subsys*/iopolicy
round-robin
round-robin
----

. Verify that the namespaces are created and correctly discovered on the host:
+
----
# nvme list
----
+
*Example output:*
+
----
Node         SN                   Model                          
---------------------------------------------------------  
/dev/nvme0n1 814vWBNRwf9HAAAAAAAB NetApp ONTAP Controller
/dev/nvme0n2 814vWBNRwf9HAAAAAAAB NetApp ONTAP Controller                                       
/dev/nvme0n3 814vWBNRwf9HAAAAAAAB NetApp ONTAP Controller                                     


Namespace Usage    Format             FW             Rev
-----------------------------------------------------------
1                 85.90 GB / 85.90 GB  4 KiB + 0 B   FFFFFFFF
2                 85.90 GB / 85.90 GB  24 KiB + 0 B  FFFFFFFF
3                 85.90 GB / 85.90 GB  4 KiB + 0 B   FFFFFFFF
----

. Verify that the controller state of each path is live and has the correct ANA status:
+
----
# nvme list-subsys /dev/nvme0n1
----
+
*Example output:*
+
----
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.5f5f2c4aa73b11e9967e00a098df41bd:subsystem.nvme_ss_ol_1
\
+- nvme0 fc traddr=nn-0x203700a098dfdd91:pn-0x203800a098dfdd91 host_traddr=nn-0x200000109b1c1204:pn-0x100000109b1c1204 live inaccessible
+- nvme1 fc traddr=nn-0x203700a098dfdd91:pn-0x203900a098dfdd91 host_traddr=nn-0x200000109b1c1204:pn-0x100000109b1c1204 live inaccessible
+- nvme2 fc traddr=nn-0x203700a098dfdd91:pn-0x203a00a098dfdd91 host_traddr=nn-0x200000109b1c1205:pn-0x100000109b1c1205 live optimized
+- nvme3 fc traddr=nn-0x203700a098dfdd91:pn-0x203d00a098dfdd91 host_traddr=nn-0x200000109b1c1205:pn-0x100000109b1c1205 live optimized
----

. Verify that the NetApp plug-in displays the correct values for each ONTAP namespace device:
+
----
# nvme netapp ontapdevices -o column
----
+
*Example output:*
+
----
Device        Vserver   Namespace Path                                                          
----------------------- ------------------------------ 
/dev/nvme0n1   vs_ol_nvme  /vol/ol_nvme_vol_1_1_0/ol_nvme_ns              
/dev/nvme0n2   vs_ol_nvme  /vol/ol_nvme_vol_1_0_0/ol_nvme_ns              
/dev/nvme0n3   vs_ol_nvme  /vol/ol_nvme_vol_1_1_1/ol_nvme_ns              


NSID       UUID                                   Size
------------------------------------------------------------
1          72b887b1-5fb6-47b8-be0b-33326e2542e2   85.90GB
2          04bf9f6e-9031-40ea-99c7-a1a61b2d7d08   85.90GB
3          264823b1-8e03-4155-80dd-e904237014a4   85.90GB
----
+
----
# nvme netapp ontapdevices -o json
{
"ONTAPdevices" : [
    {
        "Device" : "/dev/nvme0n1",
        "Vserver" : "vs_ol_nvme",
        "Namespace_Path" : "/vol/ol_nvme_vol_1_1_0/ol_nvme_ns",
        "NSID" : 1,
        "UUID" : "72b887b1-5fb6-47b8-be0b-33326e2542e2",
        "Size" : "85.90GB",
        "LBA_Data_Size" : 4096,
        "Namespace_Size" : 20971520
    },
    {
        "Device" : "/dev/nvme0n2",
        "Vserver" : "vs_ol_nvme",
        "Namespace_Path" : "/vol/ol_nvme_vol_1_0_0/ol_nvme_ns",
        "NSID" : 2,
        "UUID" : "04bf9f6e-9031-40ea-99c7-a1a61b2d7d08",
        "Size" : "85.90GB",
        "LBA_Data_Size" : 4096,
        "Namespace_Size" : 20971520
      },
      {
         "Device" : "/dev/nvme0n3",
         "Vserver" : "vs_ol_nvme",
         "Namespace_Path" : "/vol/ol_nvme_vol_1_1_1/ol_nvme_ns",
         "NSID" : 3,
         "UUID" : "264823b1-8e03-4155-80dd-e904237014a4",
         "Size" : "85.90GB",
         "LBA_Data_Size" : 4096,
         "Namespace_Size" : 20971520
       },
  ]
}
----

== Known issues

There are no known issues.

== Troubleshooting

Before commencing any troubleshooting for any NVMe/FC failures, make sure that you are running a configuration that is compliant to the Interoperability Matrix Tool (IMT) specifications and then proceed with the next steps to debug any host side issues.

=== lpfc verbose logging

include::_include/nvme/reuse_nvme_verbose_logging.adoc[]
  
=== qla2xxx verbose logging

include::_include/nvme/reuse_nvme_qla2xxx_verbose_logging.adoc[]


include::_include/hu/reuse_hu_common_nvme_cli_errors.adoc[]

=== When to contact technical support

include::_include/hu/reuse_hu_contact_tech_support.adoc[]

// 2022,12-06, Jira IEOPS-690
